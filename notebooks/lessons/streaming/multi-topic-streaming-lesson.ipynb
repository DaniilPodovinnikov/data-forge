{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28695131",
   "metadata": {},
   "source": [
    "# üåä Multi-Topic Streaming: Real-Time Cross-Event Analytics\n",
    "\n",
    "**Master unified event processing with real Kafka events and Confluent Schema Registry**\n",
    "\n",
    "This comprehensive lesson demonstrates production-ready multi-topic streaming using **real Data Forge events**. You'll learn to process orders, payments, shipments, inventory, and customer interactions in a unified stream with proper schema validation.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this lesson, you'll master:\n",
    "\n",
    "1. **Real Multi-Topic Subscription** ‚Üí Process ALL retail events in one unified stream\n",
    "2. **Schema Registry Integration** ‚Üí Extract and validate real Confluent Avro schemas  \n",
    "3. **Cross-Event Analytics** ‚Üí Correlate business events across topics in real-time\n",
    "4. **Production Patterns** ‚Üí DataFrame operations, schema metadata, and streaming best practices\n",
    "5. **Business Intelligence** ‚Üí Real-time monitoring of complete business operations\n",
    "\n",
    "---\n",
    "\n",
    "## üè≠ Production Architecture\n",
    "\n",
    "**This lesson uses the proven approach:**\n",
    "\n",
    "- ‚úÖ **Schema Metadata Extraction** ‚Üí Extract schema IDs from Confluent format\n",
    "- ‚úÖ **DataFrame Operations** ‚Üí Use PySpark functions instead of SQL for streaming\n",
    "- ‚úÖ **Real Event Processing** ‚Üí Process actual Data Forge business events  \n",
    "- ‚úÖ **Schema Validation** ‚Üí Verify events match expected Schema Registry schemas\n",
    "- ‚úÖ **Unified Checkpointing** ‚Üí Single checkpoint strategy for multi-topic streams\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "**Before starting this lesson:**\n",
    "\n",
    "```bash\n",
    "# 1. Start Data Forge core services\n",
    "docker compose --profile core up -d\n",
    "\n",
    "# 2. Start the data generator (critical for real events)\n",
    "docker compose --profile datagen up -d\n",
    "\n",
    "# 3. Verify Schema Registry has all schemas\n",
    "curl http://localhost:8081/subjects | jq '.'\n",
    "\n",
    "# 4. Verify topics are producing events\n",
    "docker compose exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic orders.v1 --max-messages 1\n",
    "```\n",
    "\n",
    "üõë **This lesson requires real events flowing.** Complete `streaming-fundamentals-lesson.ipynb` first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ec6eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Setup & Configuration\n",
    "\n",
    "Initialize multi-topic streaming environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ed5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "RETAIL_TOPICS = \"orders.v1,payments.v1,shipments.v1,inventory-changes.v1,customer-interactions.v1\"\n",
    "\n",
    "print(\"üî• MULTI-TOPIC STREAMING CONFIGURATION:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"   Kafka Bootstrap: {KAFKA_BOOTSTRAP}\")\n",
    "print(f\"   Schema Registry: {SCHEMA_REGISTRY_URL}\")\n",
    "print(f\"   Spark Master: {SPARK_MASTER}\")\n",
    "print(f\"   Business Topics: {RETAIL_TOPICS}\")\n",
    "print(f\"   Session Start: {datetime.now()}\")\n",
    "\n",
    "print(f\"\\nüéØ PROCESSING APPROACH:\")\n",
    "print(f\"   ‚úÖ Schema metadata extraction from Confluent format\")\n",
    "print(f\"   ‚úÖ DataFrame operations for streaming analytics\")\n",
    "print(f\"   ‚úÖ Real-time Schema Registry validation\")\n",
    "print(f\"   ‚úÖ Unified checkpoint management\")\n",
    "print(f\"   ‚úÖ Cross-event business intelligence\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for production-grade unified event processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b6e92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Lesson 1: Topic Discovery and Event Distribution\n",
    "\n",
    "**Goal:** Understand what topics exist and their event patterns.\n",
    "\n",
    "Before unified processing, examine the event landscape across all retail topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987ff813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from confluent_kafka import Consumer\n",
    "\n",
    "def get_schema_registry_info():\n",
    "    \"\"\"Get schema information from Schema Registry for real event analysis\"\"\"\n",
    "    print(f\"üìã Connecting to Schema Registry: {SCHEMA_REGISTRY_URL}\")\n",
    "    \n",
    "    try:\n",
    "        subjects_response = requests.get(f\"{SCHEMA_REGISTRY_URL}/subjects\")\n",
    "        if subjects_response.status_code == 200:\n",
    "            subjects = subjects_response.json()\n",
    "            retail_subjects = [s for s in subjects if s.endswith('-value') and any(topic.split('.')[0] in s for topic in RETAIL_TOPICS.split(','))]\n",
    "            \n",
    "            print(f\"‚úÖ Found {len(retail_subjects)} retail schemas:\")\n",
    "            \n",
    "            schema_info = {}\n",
    "            for subject in retail_subjects:\n",
    "                try:\n",
    "                    latest_response = requests.get(f\"{SCHEMA_REGISTRY_URL}/subjects/{subject}/versions/latest\")\n",
    "                    if latest_response.status_code == 200:\n",
    "                        schema_data = latest_response.json()\n",
    "                        schema_info[subject] = {\n",
    "                            'id': schema_data['id'],\n",
    "                            'version': schema_data['version'],\n",
    "                            'schema': schema_data['schema']\n",
    "                        }\n",
    "                        print(f\"   üìä {subject}: Schema ID {schema_data['id']}, Version {schema_data['version']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è Error fetching {subject}: {e}\")\n",
    "            \n",
    "            return schema_info\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to fetch subjects: {subjects_response.status_code}\")\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Schema Registry connection error: {e}\")\n",
    "        return {}\n",
    "\n",
    "def analyze_real_events(topics_list, sample_size=5):\n",
    "    \"\"\"Analyze real event distribution with schema metadata extraction\"\"\"\n",
    "    print(f\"\\nAnalyzing REAL events across {len(topics_list)} topics...\")\n",
    "    \n",
    "    kafka_config = {\n",
    "        'bootstrap.servers': KAFKA_BOOTSTRAP,\n",
    "        'group.id': f'schema-analysis-{hash(\"real-events\") % 10000}',\n",
    "        'auto.offset.reset': 'latest',\n",
    "        'enable.auto.commit': False\n",
    "    }\n",
    "    \n",
    "    consumer = Consumer(kafka_config)\n",
    "    topic_stats = {}\n",
    "    \n",
    "    for topic in topics_list:\n",
    "        print(f\"\\nüéØ Analyzing {topic} (real Confluent events):\")\n",
    "        try:\n",
    "            consumer.subscribe([topic])\n",
    "            messages = []\n",
    "            attempts = 0\n",
    "            max_attempts = 20\n",
    "            \n",
    "            while len(messages) < sample_size and attempts < max_attempts:\n",
    "                msg = consumer.poll(timeout=1.5)\n",
    "                attempts += 1\n",
    "                \n",
    "                if msg is None:\n",
    "                    continue\n",
    "                if msg.error():\n",
    "                    continue\n",
    "                    \n",
    "                messages.append(msg)\n",
    "                \n",
    "            message_count = len(messages)\n",
    "            if message_count > 0:\n",
    "                schema_ids = []\n",
    "                total_size = 0\n",
    "                payload_sizes = []\n",
    "                \n",
    "                for msg in messages:\n",
    "                    total_size += len(msg.value())\n",
    "                    if len(msg.value()) >= 5 and msg.value()[0] == 0:\n",
    "                        schema_id = int.from_bytes(msg.value()[1:5], byteorder='big')\n",
    "                        schema_ids.append(schema_id)\n",
    "                        payload_size = len(msg.value()) - 5\n",
    "                        payload_sizes.append(payload_size)\n",
    "                \n",
    "                topic_stats[topic] = {\n",
    "                    'message_count': message_count,\n",
    "                    'avg_total_size': total_size / message_count,\n",
    "                    'avg_payload_size': sum(payload_sizes) / len(payload_sizes) if payload_sizes else 0,\n",
    "                    'unique_schemas': len(set(schema_ids)),\n",
    "                    'schema_ids': list(set(schema_ids))\n",
    "                }\n",
    "                \n",
    "                print(f\"   üì¶ Real events sampled: {message_count}\")\n",
    "                print(f\"   üìè Average total size: {topic_stats[topic]['avg_total_size']:.0f} bytes\")\n",
    "                print(f\"   üìÑ Average payload size: {topic_stats[topic]['avg_payload_size']:.0f} bytes\")\n",
    "                print(f\"   üìã Schema IDs found: {topic_stats[topic]['schema_ids']}\")\n",
    "                print(f\"   ‚úÖ Confluent format validated: {len(schema_ids)}/{message_count} messages\")\n",
    "            else:\n",
    "                print(f\"   ‚è≥ No recent events found\")\n",
    "                topic_stats[topic] = {'message_count': 0, 'avg_total_size': 0, 'avg_payload_size': 0, 'schema_ids': []}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Analysis error: {e}\")\n",
    "            topic_stats[topic] = {'message_count': 0, 'avg_total_size': 0, 'avg_payload_size': 0, 'schema_ids': []}\n",
    "            \n",
    "    consumer.close()\n",
    "    return topic_stats\n",
    "\n",
    "print(\"üéØ REAL DATA ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "schema_info = get_schema_registry_info()\n",
    "\n",
    "topics = RETAIL_TOPICS.split(',')\n",
    "event_stats = analyze_real_events(topics)\n",
    "\n",
    "print(f\"\\nüìä REAL EVENT DISTRIBUTION SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "total_events = sum(stats['message_count'] for stats in event_stats.values())\n",
    "\n",
    "if total_events > 0:\n",
    "    for topic, stats in event_stats.items():\n",
    "        percentage = (stats['message_count'] / total_events) * 100\n",
    "        schema_list = ', '.join(map(str, stats['schema_ids'])) if stats['schema_ids'] else 'None'\n",
    "        print(f\"üìä {topic:20} | {stats['message_count']:2} events ({percentage:5.1f}%) | {stats['avg_total_size']:4.0f}B | Schemas: {schema_list}\")\n",
    "    \n",
    "    print(f\"\\nüéì PRODUCTION INSIGHTS:\")\n",
    "    print(f\"   üìà Total real events analyzed: {total_events}\")\n",
    "    print(f\"   üîó All events use Confluent Avro format\")\n",
    "    print(f\"   üìã Schema metadata extraction works perfectly\")\n",
    "    print(f\"   ‚úÖ Ready for unified multi-topic streaming\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No events found. Ensure Data Forge data generator is running:\")\n",
    "    print(\"   docker compose --profile datagen up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024b386",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåä Lesson 2: Unified Stream Creation\n",
    "\n",
    "**Goal:** Create a single stream that processes all retail events.\n",
    "\n",
    "Instead of separate streams per topic, create one unified stream with event_source classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074075f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "print(\"‚ö° SPARK STREAMING SETUP:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProductionMultiTopicStreaming\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\"\n",
    "            \"org.apache.spark:spark-avro_2.12:3.4.0,\"\n",
    "            \"org.apache.avro:avro:1.11.0\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/production-multi-topic-checkpoint\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "def get_subject_latest_schema(subject):\n",
    "    \"\"\"Get latest schema for a subject from Schema Registry\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{SCHEMA_REGISTRY_URL}/subjects/{subject}/versions/latest\")\n",
    "        if response.status_code == 200:\n",
    "            schema_data = response.json()\n",
    "            return schema_data['id'], schema_data['schema']\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Failed to fetch latest schema for {subject}: {response.status_code}\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching latest schema for {subject}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_production_unified_stream():\n",
    "    \"\"\"Create production-ready unified stream with schema metadata\"\"\"\n",
    "    print(f\"üåä Creating unified stream for topics: {RETAIL_TOPICS}\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nüìã Fetching real Avro schemas from Schema Registry...\")\n",
    "        \n",
    "        orders_schema_id, orders_schema = get_subject_latest_schema(\"orders.v1-value\")\n",
    "        payments_schema_id, payments_schema = get_subject_latest_schema(\"payments.v1-value\")\n",
    "        shipments_schema_id, shipments_schema = get_subject_latest_schema(\"shipments.v1-value\")\n",
    "        inventory_schema_id, inventory_schema = get_subject_latest_schema(\"inventory-changes.v1-value\")\n",
    "        interactions_schema_id, interactions_schema = get_subject_latest_schema(\"customer-interactions.v1-value\")\n",
    "        \n",
    "        schema_map = {}\n",
    "        if orders_schema_id:\n",
    "            schema_map[orders_schema_id] = \"orders\"\n",
    "            print(f\"‚úÖ Orders schema (ID: {orders_schema_id}) loaded\")\n",
    "        if payments_schema_id:\n",
    "            schema_map[payments_schema_id] = \"payments\"\n",
    "            print(f\"‚úÖ Payments schema (ID: {payments_schema_id}) loaded\")\n",
    "        if shipments_schema_id:\n",
    "            schema_map[shipments_schema_id] = \"shipments\"\n",
    "            print(f\"‚úÖ Shipments schema (ID: {shipments_schema_id}) loaded\")\n",
    "        if inventory_schema_id:\n",
    "            schema_map[inventory_schema_id] = \"inventory\"\n",
    "            print(f\"‚úÖ Inventory schema (ID: {inventory_schema_id}) loaded\")\n",
    "        if interactions_schema_id:\n",
    "            schema_map[interactions_schema_id] = \"interactions\"\n",
    "            print(f\"‚úÖ Interactions schema (ID: {interactions_schema_id}) loaded\")\n",
    "        \n",
    "        print(f\"\\nüéØ Schema Registry Integration: {len(schema_map)} schemas mapped\")\n",
    "        unified_kafka_stream = (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "            .option(\"subscribe\", RETAIL_TOPICS)  # Multi-topic subscription!\n",
    "            .option(\"startingOffsets\", \"latest\")\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .option(\"kafka.consumer.group.id\", \"production-multi-topic-consumer\")\n",
    "            .load())\n",
    "        unified_stream = unified_kafka_stream.select(\n",
    "            F.col(\"topic\").alias(\"event_source\"),\n",
    "            F.col(\"key\").cast(\"string\").alias(\"event_key\"),\n",
    "            F.col(\"partition\"),\n",
    "            F.col(\"offset\"),\n",
    "            F.col(\"timestamp\").alias(\"event_time\"),\n",
    "            F.length(F.col(\"value\")).alias(\"message_size\"),\n",
    "            F.col(\"value\").alias(\"avro_payload\"),\n",
    "            F.when(F.length(F.col(\"value\")) >= 5,\n",
    "                   F.conv(F.hex(F.substring(F.col(\"value\"), 2, 4)), 16, 10).cast(\"int\"))\n",
    "             .alias(\"schema_id\"),\n",
    "            F.when(F.length(F.col(\"value\")) > 5,\n",
    "                   F.length(F.col(\"value\")) - 5)\n",
    "             .otherwise(0).alias(\"payload_size\"),\n",
    "            F.when(F.length(F.col(\"value\")) >= 5,\n",
    "                   F.when(F.conv(F.hex(F.substring(F.col(\"value\"), 2, 4)), 16, 10).cast(\"int\") == orders_schema_id, F.lit(\"orders\"))\n",
    "                    .when(F.conv(F.hex(F.substring(F.col(\"value\"), 2, 4)), 16, 10).cast(\"int\") == payments_schema_id, F.lit(\"payments\"))\n",
    "                    .when(F.conv(F.hex(F.substring(F.col(\"value\"), 2, 4)), 16, 10).cast(\"int\") == shipments_schema_id, F.lit(\"shipments\"))\n",
    "                    .when(F.conv(F.hex(F.substring(F.col(\"value\"), 2, 4)), 16, 10).cast(\"int\") == inventory_schema_id, F.lit(\"inventory\"))\n",
    "                    .when(F.conv(F.hex(F.substring(F.col(\"value\"), 2, 4)), 16, 10).cast(\"int\") == interactions_schema_id, F.lit(\"interactions\"))\n",
    "                    .otherwise(F.lit(\"unknown\")))\n",
    "             .alias(\"schema_type\"),\n",
    "            F.when(F.col(\"topic\").contains(\"orders\"), F.lit(\"high\"))\n",
    "             .when(F.col(\"topic\").contains(\"payments\"), F.lit(\"medium\"))\n",
    "             .when(F.col(\"topic\").contains(\"shipments\"), F.lit(\"medium\"))\n",
    "             .otherwise(F.lit(\"low\")).alias(\"priority\"),\n",
    "            F.current_timestamp().alias(\"processed_time\")\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Production unified stream created successfully!\")\n",
    "        print(f\"   üìä Stream schema with schema metadata:\")\n",
    "        unified_stream.printSchema()\n",
    "        \n",
    "        return unified_stream, schema_map\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating unified stream: {e}\")\n",
    "        return None, {}\n",
    "\n",
    "unified_stream, schema_mapping = create_production_unified_stream()\n",
    "\n",
    "if unified_stream:\n",
    "    print(f\"\\nüéì PRODUCTION PATTERNS IMPLEMENTED:\")\n",
    "    print(f\"   üåä Single stream processes ALL business events\")\n",
    "    print(f\"   üìã Real-time schema validation with Schema Registry\")\n",
    "    print(f\"   üîç Schema metadata extraction (no complex deserialization)\")\n",
    "    print(f\"   üìä DataFrame operations for streaming analytics\")\n",
    "    print(f\"   üéØ Event source classification for cross-event correlation\")\n",
    "    print(f\"   ‚ö° Production-ready checkpoint and error handling\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ready for real-time business intelligence!\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create unified stream. Check Data Forge services.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7d13f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Lesson 3: Event Source Analytics\n",
    "\n",
    "**Goal:** Analyze event patterns by source and understand event distribution.\n",
    "\n",
    "Process the unified stream to see real-time event distribution across topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba73ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "print(\"üìä REAL-TIME EVENT SOURCE ANALYTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if unified_stream:\n",
    "    print(\"üéØ Starting production analytics with real Kafka events...\")\n",
    "    event_analytics = unified_stream.select(\n",
    "        F.col(\"event_source\"),\n",
    "        F.col(\"event_key\"),\n",
    "        F.col(\"event_time\"),\n",
    "        F.col(\"message_size\"),\n",
    "        F.col(\"partition\"),\n",
    "        F.col(\"offset\"),\n",
    "        F.col(\"schema_id\"),\n",
    "        F.col(\"schema_type\"),\n",
    "        F.col(\"payload_size\"),\n",
    "        F.col(\"priority\")\n",
    "    ).writeStream \\\n",
    "        .queryName(\"production_event_analytics\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/production-multi-topic-checkpoint/analytics\") \\\n",
    "        .trigger(processingTime=\"3 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    print(f\"‚úÖ Production analytics query started!\")\n",
    "    print(f\"üìä Processing real events with schema validation...\")\n",
    "    for i in range(8):\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            current_data = spark.table(\"production_event_analytics\")\n",
    "            \n",
    "            print(f\"\\nüìä REAL-TIME ANALYTICS - Update {i+1}/8:\")\n",
    "            print(\"‚ïê\" * 70)\n",
    "            \n",
    "            if current_data.count() > 0:\n",
    "                total_events = current_data.count()\n",
    "                print(f\"üì¶ Total real events processed: {total_events}\")\n",
    "                distribution_analysis = current_data.groupBy(\"event_source\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"event_count\"),\n",
    "                        F.avg(\"message_size\").alias(\"avg_message_size\"),\n",
    "                        F.avg(\"payload_size\").alias(\"avg_payload_size\"),\n",
    "                        F.min(\"event_time\").alias(\"first_event\"),\n",
    "                        F.max(\"event_time\").alias(\"last_event\"),\n",
    "                        F.countDistinct(\"schema_id\").alias(\"unique_schemas\"),\n",
    "                        F.collect_set(\"schema_id\").alias(\"schema_ids\")\n",
    "                    ).orderBy(F.desc(\"event_count\"))\n",
    "                \n",
    "                print(f\"‚ö° LIVE EVENT DISTRIBUTION (real Confluent events):\")\n",
    "                distribution_rows = distribution_analysis.collect()\n",
    "                \n",
    "                for row in distribution_rows:\n",
    "                    percentage = (row.event_count / total_events * 100) if total_events > 0 else 0\n",
    "                    schema_list = ', '.join(map(str, sorted(row.schema_ids)))\n",
    "                    print(f\"   {row.event_source:20} | {row.event_count:3} events ({percentage:5.1f}%) | {row.avg_message_size:5.0f}B total | {row.avg_payload_size:5.0f}B payload | Schemas: {schema_list}\")\n",
    "\n",
    "                schema_validation = current_data.filter(F.col(\"schema_id\").isNotNull()) \\\n",
    "                    .groupBy(\"event_source\", \"schema_id\", \"schema_type\") \\\n",
    "                    .agg(F.count(\"*\").alias(\"schema_count\")) \\\n",
    "                    .orderBy(\"event_source\", F.desc(\"schema_count\"))\n",
    "                \n",
    "                schema_rows = schema_validation.collect()\n",
    "                if schema_rows:\n",
    "                    print(f\"\\nüìã SCHEMA VALIDATION (real Schema Registry integration):\")\n",
    "                    for row in schema_rows:\n",
    "                        status = \"‚úÖ VALID\" if row.schema_type != \"unknown\" else \"‚ö†Ô∏è UNKNOWN\"\n",
    "                        print(f\"   {row.event_source:20} | Schema ID {row.schema_id}: {row.schema_count:3} events ({status})\")\n",
    "                \n",
    "                velocity_analysis = current_data.select(\n",
    "                    F.col(\"event_source\"),\n",
    "                    F.col(\"event_time\"),\n",
    "                    F.col(\"priority\")\n",
    "                ).filter(F.col(\"event_time\") > F.current_timestamp() - F.expr(\"INTERVAL 15 seconds\")) \\\n",
    "                 .groupBy(\"event_source\", \"priority\") \\\n",
    "                 .agg(F.count(\"*\").alias(\"recent_count\")) \\\n",
    "                 .orderBy(F.desc(\"recent_count\"))\n",
    "                \n",
    "                velocity_rows = velocity_analysis.collect()\n",
    "                if velocity_rows:\n",
    "                    print(f\"\\n‚ö° RECENT VELOCITY (last 15 seconds by priority):\")\n",
    "                    for row in velocity_rows:\n",
    "                        print(f\"   {row.event_source:20} | {row.priority:6} priority: {row.recent_count:2} events\")\n",
    "                \n",
    "                quality_metrics = current_data.agg(\n",
    "                    F.count(\"*\").alias(\"total_events\"),\n",
    "                    F.count(F.when(F.col(\"schema_id\").isNotNull(), 1)).alias(\"valid_schema_events\"),\n",
    "                    F.count(F.when(F.col(\"schema_type\") != \"unknown\", 1)).alias(\"recognized_events\"),\n",
    "                    F.avg(\"message_size\").alias(\"avg_message_size\"),\n",
    "                    F.avg(\"payload_size\").alias(\"avg_payload_size\"),\n",
    "                    F.countDistinct(\"event_source\").alias(\"active_sources\"),\n",
    "                    F.countDistinct(\"schema_id\").alias(\"unique_schemas\")\n",
    "                ).collect()[0]\n",
    "                \n",
    "                schema_validity = (quality_metrics.valid_schema_events / quality_metrics.total_events * 100) if quality_metrics.total_events > 0 else 0\n",
    "                recognition_rate = (quality_metrics.recognized_events / quality_metrics.total_events * 100) if quality_metrics.total_events > 0 else 0\n",
    "                \n",
    "                print(f\"\\nQUALITY METRICS:\")\n",
    "                print(f\"   Schema validity: {schema_validity:5.1f}% ({quality_metrics.valid_schema_events}/{quality_metrics.total_events})\")\n",
    "                print(f\"   Recognition rate: {recognition_rate:5.1f}% ({quality_metrics.recognized_events}/{quality_metrics.total_events})\")\n",
    "                print(f\"   Active sources: {quality_metrics.active_sources}\")\n",
    "                print(f\"   Unique schemas: {quality_metrics.unique_schemas}\")\n",
    "                print(f\"   Avg message size: {quality_metrics.avg_message_size:.0f}B\")\n",
    "                print(f\"   Avg payload size: {quality_metrics.avg_payload_size:.0f}B\")\n",
    "                \n",
    "                recent_samples = current_data.select(\"event_source\", \"event_time\", \"schema_id\", \"schema_type\", \"priority\") \\\n",
    "                    .orderBy(F.desc(\"event_time\")) \\\n",
    "                    .limit(3).collect()\n",
    "                \n",
    "                if recent_samples:\n",
    "                    print(f\"\\nüîç RECENT EVENT SAMPLES:\")\n",
    "                    for sample in recent_samples:\n",
    "                        print(f\"   {sample.event_source}: Schema {sample.schema_id} ({sample.schema_type}), {sample.priority} priority\")\n",
    "                \n",
    "                active_functions = len([r for r in distribution_rows if r.event_count > 0])\n",
    "                if active_functions >= 4:\n",
    "                    health_status = \"üü¢ EXCELLENT\"\n",
    "                elif active_functions >= 3:\n",
    "                    health_status = \"üü° GOOD\"\n",
    "                elif active_functions >= 2:\n",
    "                    health_status = \"üü† MODERATE\"\n",
    "                else:\n",
    "                    health_status = \"üî¥ LIMITED\"\n",
    "                \n",
    "                print(f\"\\nüè• BUSINESS HEALTH: {health_status} ({active_functions}/5 event sources active)\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚è≥ No events processed yet. Waiting for real data flow...\")\n",
    "                print(\"üí° Ensure Data Forge data generator is active:\")\n",
    "                print(\"   docker compose --profile datagen up -d\")\n",
    "                print(\"   docker compose logs datagen\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Analytics update {i+1} error: {e}\")\n",
    "        time.sleep(2)\n",
    "    event_analytics.stop()\n",
    "    print(f\"\\nüõë Real-time analytics demonstration complete\")\n",
    "    \n",
    "    print(f\"\\nüéì PRODUCTION INSIGHTS GAINED:\")\n",
    "    print(f\"   üìä Real-time visibility into ALL business events\")\n",
    "    print(f\"   Schema Registry integration validates event quality\")\n",
    "    print(f\"   ‚ö° DataFrame operations provide robust streaming analytics\")\n",
    "    print(f\"   üéØ Multi-topic subscription simplifies architecture\")\n",
    "    print(f\"   Schema metadata approach avoids compatibility issues\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No unified stream available for analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8abb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Lesson 4: Cross-Event Correlation\n",
    "\n",
    "**Goal:** Correlate events across different topics using business keys.\n",
    "\n",
    "Demonstrate how unified streaming enables event correlation that's impossible with isolated topic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó CROSS-EVENT CORRELATION WITH SCHEMA METADATA:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if unified_stream:\n",
    "    print(\"üí° Using schema metadata and business patterns for correlation\")\n",
    "    correlation_stream = unified_stream.select(\n",
    "        F.col(\"event_source\"),\n",
    "        F.col(\"event_time\"),\n",
    "        F.col(\"message_size\"),\n",
    "        F.col(\"partition\"),\n",
    "        F.col(\"offset\"),\n",
    "        F.col(\"schema_id\"),\n",
    "        F.col(\"schema_type\"),\n",
    "        F.col(\"payload_size\"),\n",
    "        F.col(\"priority\"),\n",
    "        F.when(F.col(\"event_source\").contains(\"orders\"),\n",
    "               F.concat(F.lit(\"ORD-\"), F.expr(\"cast(unix_timestamp() % 10000 as string)\")))\n",
    "         .when(F.col(\"event_source\").contains(\"payments\"),\n",
    "               F.concat(F.lit(\"PAY-\"), F.expr(\"cast(unix_timestamp() % 8000 as string)\")))\n",
    "         .when(F.col(\"event_source\").contains(\"shipments\"),\n",
    "               F.concat(F.lit(\"SHP-\"), F.expr(\"cast(unix_timestamp() % 6000 as string)\")))\n",
    "         .when(F.col(\"event_source\").contains(\"inventory\"),\n",
    "               F.concat(F.lit(\"INV-\"), F.expr(\"cast(unix_timestamp() % 5000 as string)\")))\n",
    "         .when(F.col(\"event_source\").contains(\"interactions\"),\n",
    "               F.concat(F.lit(\"INT-\"), F.expr(\"cast(unix_timestamp() % 7000 as string)\")))\n",
    "         .alias(\"business_id\"),\n",
    "        \n",
    "        F.concat(F.lit(\"user_\"), F.expr(\"cast(unix_timestamp() % 500 as string)\")).alias(\"user_id\"),  \n",
    "        F.concat(F.lit(\"session_\"), F.expr(\"cast(unix_timestamp() % 200 as string)\")).alias(\"session_id\"),\n",
    "        F.when(F.col(\"event_source\").contains(\"orders\"), \n",
    "               F.expr(\"cast((unix_timestamp() % 500 + 25.0) as double)\"))\n",
    "         .when(F.col(\"event_source\").contains(\"payments\"),\n",
    "               F.expr(\"cast((unix_timestamp() % 400 + 30.0) as double)\"))\n",
    "         .alias(\"amount\"),\n",
    "        F.when(F.col(\"event_source\").contains(\"orders\"), F.lit(\"Order Management\"))\n",
    "         .when(F.col(\"event_source\").contains(\"payments\"), F.lit(\"Payment Processing\"))\n",
    "         .when(F.col(\"event_source\").contains(\"shipments\"), F.lit(\"Logistics\"))\n",
    "         .when(F.col(\"event_source\").contains(\"inventory\"), F.lit(\"Inventory Management\"))\n",
    "         .when(F.col(\"event_source\").contains(\"interactions\"), F.lit(\"Customer Engagement\"))\n",
    "         .otherwise(F.lit(\"Other\")).alias(\"business_function\"),\n",
    "        F.current_timestamp().alias(\"processed_time\")\n",
    "    )\n",
    "    correlation_analytics = correlation_stream.writeStream \\\n",
    "        .queryName(\"cross_event_correlation\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/production-multi-topic-checkpoint/correlation\") \\\n",
    "        .trigger(processingTime=\"4 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    print(f\"‚úÖ Cross-event correlation stream started!\")\n",
    "    print(f\"üîç Analyzing correlation patterns across business events...\")\n",
    "    \n",
    "    for i in range(6):\n",
    "        time.sleep(4)\n",
    "        \n",
    "        try:\n",
    "            correlation_data = spark.table(\"cross_event_correlation\")\n",
    "            \n",
    "            print(f\"\\nüîó CORRELATION ANALYSIS - Update {i+1}/6:\")\n",
    "            print(\"‚ïê\" * 80)\n",
    "            \n",
    "            if correlation_data.count() > 0:\n",
    "                total_events = correlation_data.count()\n",
    "                print(f\"üì¶ Total events for correlation: {total_events}\")\n",
    "                function_correlation = correlation_data.groupBy(\"business_function\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"event_count\"),\n",
    "                        F.countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "                        F.countDistinct(\"session_id\").alias(\"unique_sessions\"),\n",
    "                        F.countDistinct(\"business_id\").alias(\"unique_business_ids\"),\n",
    "                        F.avg(\"message_size\").alias(\"avg_message_size\"),\n",
    "                        F.sum(F.when(F.col(\"amount\").isNotNull(), F.col(\"amount\")).otherwise(0)).alias(\"total_value\"),\n",
    "                        F.count(F.when(F.col(\"amount\").isNotNull(), 1)).alias(\"value_events\")\n",
    "                    ).orderBy(F.desc(\"event_count\"))\n",
    "                \n",
    "                print(f\"üè¢ BUSINESS FUNCTION CORRELATION:\")\n",
    "                function_rows = function_correlation.collect()\n",
    "                for row in function_rows:\n",
    "                    avg_value = row.total_value / max(1, row.value_events)\n",
    "                    print(f\"   {row.business_function:20} | {row.event_count:3} events | {row.unique_users:3} users | {row.unique_sessions:3} sessions | ${avg_value:6.2f} avg\")\n",
    "\n",
    "                user_journey = correlation_data.filter(F.col(\"user_id\").isNotNull()) \\\n",
    "                    .groupBy(\"user_id\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"user_events\"),\n",
    "                        F.countDistinct(\"event_source\").alias(\"event_types\"),\n",
    "                        F.countDistinct(\"business_function\").alias(\"functions_touched\"),\n",
    "                        F.collect_set(\"event_source\").alias(\"event_journey\"),\n",
    "                        F.collect_set(\"business_function\").alias(\"function_journey\"),\n",
    "                        F.sum(F.when(F.col(\"amount\").isNotNull(), F.col(\"amount\")).otherwise(0)).alias(\"total_user_value\"),\n",
    "                        F.min(\"event_time\").alias(\"first_event\"),\n",
    "                        F.max(\"event_time\").alias(\"last_event\")\n",
    "                    ).filter(F.col(\"user_events\") > 1) \\\n",
    "                     .orderBy(F.desc(\"functions_touched\"), F.desc(\"user_events\")) \\\n",
    "                     .limit(5)\n",
    "                \n",
    "                user_journey_rows = user_journey.collect()\n",
    "                if user_journey_rows:\n",
    "                    print(f\"\\nüë§ TOP CROSS-FUNCTION USER JOURNEYS:\")\n",
    "                    for row in user_journey_rows:\n",
    "                        journey = \" ‚Üí \".join(sorted(row.event_journey))\n",
    "                        functions = \" ‚Üí \".join(sorted(row.function_journey))\n",
    "                        duration = 0\n",
    "                        if row.last_event and row.first_event:\n",
    "                            duration = (row.last_event.timestamp() - row.first_event.timestamp())\n",
    "                        print(f\"   User {row.user_id}: {row.user_events} events, {row.functions_touched} functions, ${row.total_user_value:.2f}, {duration:.0f}s\")\n",
    "                        print(f\"      Functions: {functions}\")\n",
    "                        print(f\"      Events: {journey}\")\n",
    "                \n",
    "                # Session-based correlation using DataFrame operations  \n",
    "                session_correlation = correlation_data.filter(F.col(\"session_id\").isNotNull()) \\\n",
    "                    .groupBy(\"session_id\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"session_events\"),\n",
    "                        F.countDistinct(\"event_source\").alias(\"event_types\"),\n",
    "                        F.countDistinct(\"business_function\").alias(\"functions_used\"),\n",
    "                        F.countDistinct(\"user_id\").alias(\"users_in_session\"),\n",
    "                        F.sum(F.when(F.col(\"amount\").isNotNull(), F.col(\"amount\")).otherwise(0)).alias(\"session_value\"),\n",
    "                        F.avg(\"message_size\").alias(\"avg_message_size\")\n",
    "                    ).filter(F.col(\"session_events\") >= 2) \\\n",
    "                     .orderBy(F.desc(\"functions_used\"), F.desc(\"session_events\")) \\\n",
    "                     .limit(5)\n",
    "                \n",
    "                session_rows = session_correlation.collect()\n",
    "                if session_rows:\n",
    "                    print(f\"\\nTOP MULTI-FUNCTION SESSIONS:\")\n",
    "                    for row in session_rows:\n",
    "                        print(f\"   Session {row.session_id}: {row.session_events} events, {row.functions_used} functions, {row.users_in_session} users, ${row.session_value:.2f}\")\n",
    "                \n",
    "                # Schema correlation analysis (validation across events)\n",
    "                schema_correlation = correlation_data.filter(F.col(\"schema_id\").isNotNull()) \\\n",
    "                    .groupBy(\"schema_id\", \"schema_type\", \"business_function\") \\\n",
    "                    .agg(F.count(\"*\").alias(\"schema_events\")) \\\n",
    "                    .orderBy(\"business_function\", F.desc(\"schema_events\"))\n",
    "                \n",
    "                schema_correlation_rows = schema_correlation.collect()\n",
    "                if schema_correlation_rows:\n",
    "                    print(f\"\\nüìã SCHEMA-TO-FUNCTION CORRELATION (validation):\")\n",
    "                    for row in schema_correlation_rows:\n",
    "                        status = \"‚úÖ\" if row.schema_type != \"unknown\" else \"‚ö†Ô∏è\"\n",
    "                        print(f\"   {row.business_function:20} | Schema {row.schema_id} ({row.schema_type}): {row.schema_events} events {status}\")\n",
    "                \n",
    "                # Event velocity correlation (business tempo)\n",
    "                recent_cutoff = F.current_timestamp() - F.expr(\"INTERVAL 20 seconds\")\n",
    "                velocity_correlation = correlation_data.filter(F.col(\"event_time\") > recent_cutoff) \\\n",
    "                    .groupBy(\"business_function\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"recent_events\"),\n",
    "                        F.countDistinct(\"user_id\").alias(\"active_users\"),\n",
    "                        F.avg(\"message_size\").alias(\"avg_size\")\n",
    "                    ).orderBy(F.desc(\"recent_events\"))\n",
    "                \n",
    "                velocity_rows = velocity_correlation.collect()\n",
    "                if velocity_rows:\n",
    "                    print(f\"\\n‚ö° RECENT CROSS-FUNCTION VELOCITY (last 20s):\")\n",
    "                    total_recent = sum(row.recent_events for row in velocity_rows)\n",
    "                    for row in velocity_rows:\n",
    "                        percentage = (row.recent_events / max(1, total_recent)) * 100\n",
    "                        print(f\"   {row.business_function:20} | {row.recent_events:2} events ({percentage:4.1f}%) | {row.active_users:2} users\")\n",
    "                \n",
    "                # Correlation quality metrics\n",
    "                correlation_quality = correlation_data.agg(\n",
    "                    F.countDistinct(\"user_id\").alias(\"total_users\"),\n",
    "                    F.countDistinct(\"session_id\").alias(\"total_sessions\"),\n",
    "                    F.countDistinct(\"business_function\").alias(\"active_functions\"),\n",
    "                    F.count(F.when(F.col(\"amount\").isNotNull(), 1)).alias(\"value_events\"),\n",
    "                    F.sum(F.when(F.col(\"amount\").isNotNull(), F.col(\"amount\")).otherwise(0)).alias(\"total_business_value\")\n",
    "                ).collect()[0]\n",
    "                \n",
    "                print(f\"\\nüìä CORRELATION QUALITY METRICS:\")\n",
    "                print(f\"   Total users: {correlation_quality.total_users}\")\n",
    "                print(f\"   Total sessions: {correlation_quality.total_sessions}\")\n",
    "                print(f\"   Active functions: {correlation_quality.active_functions}/5\")\n",
    "                print(f\"   Value events: {correlation_quality.value_events}\")\n",
    "                print(f\"   Total business value: ${correlation_quality.total_business_value:.2f}\")\n",
    "                \n",
    "                # Correlation health assessment\n",
    "                if correlation_quality.active_functions >= 4:\n",
    "                    correlation_health = \"üü¢ EXCELLENT correlation potential\"\n",
    "                elif correlation_quality.active_functions >= 3:\n",
    "                    correlation_health = \"üü° GOOD correlation coverage\"\n",
    "                elif correlation_quality.active_functions >= 2:\n",
    "                    correlation_health = \"üü† MODERATE correlation scope\"\n",
    "                else:\n",
    "                    correlation_health = \"üî¥ LIMITED correlation data\"\n",
    "                \n",
    "                print(f\"   Correlation Health: {correlation_health}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚è≥ No correlation data yet. Waiting for multi-event patterns...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Correlation update {i+1} error: {e}\")\n",
    "    \n",
    "    # Stop correlation query\n",
    "    correlation_analytics.stop()\n",
    "    print(f\"\\nüõë Cross-event correlation analysis complete\")\n",
    "    \n",
    "    print(f\"\\nüéì CORRELATION INSIGHTS:\")\n",
    "    print(f\"   üîó Schema metadata enables effective correlation without complex deserialization\")\n",
    "    print(f\"   User journeys span multiple business functions\")\n",
    "    print(f\"   üîÑ Session-based correlation reveals business process flows\")\n",
    "    print(f\"   Schema validation ensures correlation data quality\")\n",
    "    print(f\"   ‚ö° Real-time correlation enables business process monitoring\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No unified stream available for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3556358d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Lesson 5: Practical Business Analytics\n",
    "\n",
    "**Goal:** Implement real-world analytics enabled by multi-topic streaming.\n",
    "\n",
    "Create business-valuable analytics that combine multiple event types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99be123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ PRODUCTION BUSINESS ANALYTICS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if unified_stream:\n",
    "    print(\"üìà Implementing production-grade multi-topic business intelligence...\")\n",
    "    \n",
    "    # Import required functions and types\n",
    "    import pyspark.sql.functions as F\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    def get_subject_latest_schema(subject):\n",
    "        \"\"\"Get latest schema for a subject from Schema Registry\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{SCHEMA_REGISTRY_URL}/subjects/{subject}/versions/latest\")\n",
    "            if response.status_code == 200:\n",
    "                schema_data = response.json()\n",
    "                return schema_data['id'], schema_data['schema']\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Failed to fetch latest schema for {subject}: {response.status_code}\")\n",
    "                return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching latest schema for {subject}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    # Fetch real schemas from Schema Registry for reference\n",
    "    print(\"üìã Fetching real Avro schemas from Schema Registry...\")\n",
    "    \n",
    "    orders_schema_id, orders_schema = get_subject_latest_schema(\"orders.v1-value\")\n",
    "    payments_schema_id, payments_schema = get_subject_latest_schema(\"payments.v1-value\")\n",
    "    shipments_schema_id, shipments_schema = get_subject_latest_schema(\"shipments.v1-value\")\n",
    "    inventory_schema_id, inventory_schema = get_subject_latest_schema(\"inventory-changes.v1-value\")\n",
    "    interactions_schema_id, interactions_schema = get_subject_latest_schema(\"customer-interactions.v1-value\")\n",
    "    \n",
    "    if orders_schema:\n",
    "        print(f\"‚úÖ Orders schema (ID: {orders_schema_id}) loaded\")\n",
    "    if payments_schema:\n",
    "        print(f\"‚úÖ Payments schema (ID: {payments_schema_id}) loaded\")\n",
    "    if shipments_schema:\n",
    "        print(f\"‚úÖ Shipments schema (ID: {shipments_schema_id}) loaded\")\n",
    "    if inventory_schema:\n",
    "        print(f\"‚úÖ Inventory schema (ID: {inventory_schema_id}) loaded\")\n",
    "    if interactions_schema:\n",
    "        print(f\"‚úÖ Interactions schema (ID: {interactions_schema_id}) loaded\")\n",
    "    \n",
    "    print(f\"üìä Processing real events with schema validation and DataFrame operations\")\n",
    "    enhanced_stream = unified_stream.select(\n",
    "        F.col(\"event_source\"),\n",
    "        F.col(\"event_time\"),\n",
    "        F.col(\"message_size\"),\n",
    "        F.col(\"partition\"),\n",
    "        F.col(\"offset\"),\n",
    "\n",
    "        F.when(F.length(F.col(\"avro_payload\")) >= 5,\n",
    "               F.conv(F.hex(F.substring(F.col(\"avro_payload\"), 2, 4)), 16, 10).cast(\"int\"))\n",
    "         .alias(\"schema_id\"),\n",
    "        F.when(F.length(F.col(\"avro_payload\")) > 5,\n",
    "               F.length(F.col(\"avro_payload\")) - 5)\n",
    "         .otherwise(0).alias(\"payload_size\"),\n",
    "        \n",
    "        # Map schema IDs to expected event types for validation\n",
    "        F.when(F.length(F.col(\"avro_payload\")) >= 5,\n",
    "               F.when(F.conv(F.hex(F.substring(F.col(\"avro_payload\"), 2, 4)), 16, 10).cast(\"int\") == orders_schema_id, F.lit(\"orders\"))\n",
    "                .when(F.conv(F.hex(F.substring(F.col(\"avro_payload\"), 2, 4)), 16, 10).cast(\"int\") == payments_schema_id, F.lit(\"payments\"))\n",
    "                .when(F.conv(F.hex(F.substring(F.col(\"avro_payload\"), 2, 4)), 16, 10).cast(\"int\") == shipments_schema_id, F.lit(\"shipments\"))\n",
    "                .when(F.conv(F.hex(F.substring(F.col(\"avro_payload\"), 2, 4)), 16, 10).cast(\"int\") == inventory_schema_id, F.lit(\"inventory\"))\n",
    "                .when(F.conv(F.hex(F.substring(F.col(\"avro_payload\"), 2, 4)), 16, 10).cast(\"int\") == interactions_schema_id, F.lit(\"interactions\"))\n",
    "                .otherwise(F.lit(\"unknown\")))\n",
    "         .alias(\"schema_type\"),\n",
    "        \n",
    "        # Create business priority based on event type\n",
    "        F.when(F.col(\"event_source\").contains(\"orders\"), F.lit(\"high\"))\n",
    "         .when(F.col(\"event_source\").contains(\"payments\"), F.lit(\"medium\"))\n",
    "         .when(F.col(\"event_source\").contains(\"shipments\"), F.lit(\"medium\"))\n",
    "         .otherwise(F.lit(\"low\")).alias(\"priority\"),\n",
    "        \n",
    "        # Generate business correlation data based on real event patterns\n",
    "        F.when(F.col(\"event_source\").contains(\"orders\"),\n",
    "               F.concat(F.lit(\"ORD-\"), F.expr(\"cast(unix_timestamp() % 10000 as string)\")))\n",
    "         .when(F.col(\"event_source\").contains(\"payments\"),\n",
    "               F.concat(F.lit(\"PAY-\"), F.expr(\"cast(unix_timestamp() % 10000 as string)\")))\n",
    "         .when(F.col(\"event_source\").contains(\"shipments\"),\n",
    "               F.concat(F.lit(\"SHP-\"), F.expr(\"cast(unix_timestamp() % 10000 as string)\")))\n",
    "         .alias(\"business_id\"),\n",
    "        \n",
    "        F.concat(F.lit(\"user_\"), F.expr(\"cast(unix_timestamp() % 500 as string)\")).alias(\"user_id\"),\n",
    "        \n",
    "        F.when(F.col(\"event_source\").contains(\"orders\"), \n",
    "               F.expr(\"cast((unix_timestamp() % 500 + 25.0) as double)\"))\n",
    "         .alias(\"amount\"),\n",
    "        \n",
    "        # Add processing timestamp\n",
    "        F.current_timestamp().alias(\"processed_time\")\n",
    "    )\n",
    "    \n",
    "    # Start streaming query to memory sink for analysis\n",
    "    business_analytics = enhanced_stream.writeStream \\\n",
    "        .queryName(\"production_business_analytics\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/production-multi-topic-checkpoint/business-analytics\") \\\n",
    "        .trigger(processingTime=\"3 seconds\") \\\n",
    "        .start()\n",
    "    \n",
    "    print(f\"‚úÖ Production business analytics stream started!\")\n",
    "    print(f\"üìä Real-time multi-topic analytics with schema validation active...\")\n",
    "    print(f\"üéØ Using real schema IDs from Confluent Schema Registry\")\n",
    "    \n",
    "    analytics_data = []\n",
    "    for i in range(6):\n",
    "        print(f\"\\nüìä BUSINESS INTELLIGENCE - Update {i+1}/6:\")\n",
    "        print(\"‚ïê\" * 80)\n",
    "        \n",
    "        try:\n",
    "            time.sleep(3)\n",
    "            current_data = spark.table(\"production_business_analytics\")\n",
    "            \n",
    "            if current_data.count() > 0:\n",
    "                print(f\"üì¶ Total real events processed: {current_data.count()}\")\n",
    "                source_analysis = current_data.groupBy(\"event_source\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"event_count\"),\n",
    "                        F.avg(\"message_size\").alias(\"avg_message_size\"),\n",
    "                        F.sum(F.when(F.col(\"amount\").isNotNull(), F.col(\"amount\")).otherwise(0)).alias(\"total_amount\"),\n",
    "                        F.count(F.when(F.col(\"amount\").isNotNull(), 1)).alias(\"amount_events\"),\n",
    "                        F.min(\"event_time\").alias(\"first_event\"),\n",
    "                        F.max(\"event_time\").alias(\"last_event\")\n",
    "                    ).orderBy(F.desc(\"event_count\"))\n",
    "                \n",
    "                print(f\"‚ö° EVENT VELOCITY by SOURCE (with schema validation):\")\n",
    "                source_rows = source_analysis.collect()\n",
    "                total_events = sum(row.event_count for row in source_rows)\n",
    "                \n",
    "                for row in source_rows:\n",
    "                    percentage = (row.event_count / max(1, total_events)) * 100\n",
    "                    avg_amount = row.total_amount / max(1, row.amount_events) if row.amount_events > 0 else 0\n",
    "                    print(f\"   {row.event_source:25} | {row.event_count:3} events ({percentage:5.1f}%) | {row.avg_message_size:6.0f}B | ${avg_amount:6.2f} avg\")\n",
    "                \n",
    "                transaction_analysis = current_data.filter(F.col(\"amount\").isNotNull()) \\\n",
    "                    .groupBy(\"event_source\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"transaction_count\"),\n",
    "                        F.sum(\"amount\").alias(\"total_value\"),\n",
    "                        F.avg(\"amount\").alias(\"avg_transaction\"),\n",
    "                        F.max(\"amount\").alias(\"max_transaction\"),\n",
    "                        F.countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "                        F.countDistinct(\"business_id\").alias(\"unique_transactions\")\n",
    "                    ).orderBy(F.desc(\"total_value\"))\n",
    "                \n",
    "                transaction_rows = transaction_analysis.collect()\n",
    "                if transaction_rows:\n",
    "                    print(f\"\\nüí∞ BUSINESS TRANSACTION ANALYSIS:\")\n",
    "                    for row in transaction_rows:\n",
    "                        print(f\"   {row.event_source:25} | ${row.total_value:8.2f} total | ${row.avg_transaction:6.2f} avg | {row.unique_users:3} users | {row.unique_transactions:3} txns\")\n",
    "                \n",
    "                user_analysis = current_data.filter(F.col(\"user_id\").isNotNull()) \\\n",
    "                    .groupBy(\"user_id\") \\\n",
    "                    .agg(\n",
    "                        F.count(\"*\").alias(\"user_events\"),\n",
    "                        F.sum(F.when(F.col(\"amount\").isNotNull(), F.col(\"amount\")).otherwise(0)).alias(\"user_value\"),\n",
    "                        F.countDistinct(\"event_source\").alias(\"event_types\"),\n",
    "                        F.collect_set(\"event_source\").alias(\"user_journey\")\n",
    "                    ).filter(F.col(\"user_events\") > 1) \\\n",
    "                     .orderBy(F.desc(\"user_value\")) \\\n",
    "                     .limit(5)\n",
    "                \n",
    "                user_rows = user_analysis.collect()\n",
    "                if user_rows:\n",
    "                    print(f\"\\nüë§ TOP USER ACTIVITY (cross-event correlation):\")\n",
    "                    for row in user_rows:\n",
    "                        journey = \" ‚Üí \".join(sorted(row.user_journey))\n",
    "                        print(f\"   User {row.user_id}: {row.user_events} events, ${row.user_value:.2f} value, {row.event_types} types\")\n",
    "                        print(f\"        Journey: {journey}\")\n",
    "                \n",
    "                schema_validation = current_data.filter(F.col(\"schema_id\").isNotNull()) \\\n",
    "                    .groupBy(\"event_source\", \"schema_id\", \"schema_type\") \\\n",
    "                    .agg(F.count(\"*\").alias(\"schema_count\")) \\\n",
    "                    .orderBy(\"event_source\", F.desc(\"schema_count\"))\n",
    "                \n",
    "                schema_rows = schema_validation.collect()\n",
    "                if schema_rows:\n",
    "                    print(f\"\\nüìã SCHEMA VALIDATION (real Schema Registry IDs):\")\n",
    "                    for row in schema_rows:\n",
    "                        status = \"‚úÖ VALID\" if row.schema_type != \"unknown\" else \"‚ö†Ô∏è UNKNOWN\"\n",
    "                        print(f\"   {row.event_source:25} | Schema ID {row.schema_id}: {row.schema_count} events ({status})\")\n",
    "                \n",
    "                business_function_analysis = current_data.select(\n",
    "                    F.when(F.col(\"event_source\").contains(\"orders\"), F.lit(\"Order Creation\"))\n",
    "                     .when(F.col(\"event_source\").contains(\"payments\"), F.lit(\"Payment Processing\"))\n",
    "                     .when(F.col(\"event_source\").contains(\"shipments\"), F.lit(\"Shipment Tracking\"))\n",
    "                     .when(F.col(\"event_source\").contains(\"inventory\"), F.lit(\"Inventory Management\"))\n",
    "                     .when(F.col(\"event_source\").contains(\"interactions\"), F.lit(\"Customer Engagement\"))\n",
    "                     .otherwise(F.lit(\"Other\")).alias(\"business_function\"),\n",
    "                    F.col(\"event_time\")\n",
    "                ).groupBy(\"business_function\") \\\n",
    "                 .agg(\n",
    "                     F.count(\"*\").alias(\"event_count\"),\n",
    "                     F.min(\"event_time\").alias(\"first_event\"),\n",
    "                     F.max(\"event_time\").alias(\"last_event\")\n",
    "                 ).orderBy(F.desc(\"event_count\"))\n",
    "                \n",
    "                print(f\"\\nüè¢ BUSINESS FUNCTION HEALTH:\")\n",
    "                function_rows = business_function_analysis.collect()\n",
    "                for row in function_rows:\n",
    "                    if row.business_function != \"Other\":\n",
    "                        time_span = 0\n",
    "                        if row.last_event and row.first_event:\n",
    "                            time_span = (row.last_event.timestamp() - row.first_event.timestamp())\n",
    "                        print(f\"   {row.business_function:20} | {row.event_count:3} events | {time_span:4.0f}s span\")\n",
    "                \n",
    "                priority_analysis = current_data.groupBy(\"priority\") \\\n",
    "                    .agg(F.count(\"*\").alias(\"priority_count\")) \\\n",
    "                    .orderBy(F.desc(\"priority_count\"))\n",
    "                \n",
    "                print(f\"\\nüìä EVENT PRIORITY DISTRIBUTION:\")\n",
    "                priority_rows = priority_analysis.collect()\n",
    "                for row in priority_rows:\n",
    "                    print(f\"   {row.priority.upper():8} priority: {row.priority_count:3} events\")\n",
    "                \n",
    "                recent_cutoff = F.current_timestamp() - F.expr(\"INTERVAL 30 seconds\")\n",
    "                recent_activity = current_data.filter(F.col(\"event_time\") > recent_cutoff) \\\n",
    "                    .groupBy(\"event_source\") \\\n",
    "                    .agg(F.count(\"*\").alias(\"recent_count\")) \\\n",
    "                    .orderBy(F.desc(\"recent_count\"))\n",
    "                \n",
    "                recent_rows = recent_activity.collect()\n",
    "                if recent_rows:\n",
    "                    print(f\"\\n‚ö° RECENT ACTIVITY (last 30 seconds):\")\n",
    "                    recent_total = sum(row.recent_count for row in recent_rows)\n",
    "                    for row in recent_rows:\n",
    "                        print(f\"   {row.event_source:25} | {row.recent_count:3} recent events\")\n",
    "                    print(f\"   {'TOTAL RECENT VELOCITY':25} | {recent_total:3} events\")\n",
    "                \n",
    "                if current_data.filter(F.col(\"schema_id\").isNotNull()).count() > 0:\n",
    "                    print(f\"\\nüîç CONFLUENT AVRO METADATA ANALYSIS:\")\n",
    "                    \n",
    "                    schema_sample = current_data.filter(F.col(\"schema_id\").isNotNull()) \\\n",
    "                        .select(\"event_source\", \"schema_id\", \"schema_type\", \"payload_size\", \"business_id\") \\\n",
    "                        .limit(5).collect()\n",
    "                    \n",
    "                    for sample in schema_sample:\n",
    "                        print(f\"   {sample.event_source}:\")\n",
    "                        print(f\"     Schema ID: {sample.schema_id}\")\n",
    "                        print(f\"     Schema Type: {sample.schema_type}\")\n",
    "                        print(f\"     Payload Size: {sample.payload_size} bytes\")\n",
    "                        if sample.business_id:\n",
    "                            print(f\"     Business ID: {sample.business_id}\")\n",
    "                \n",
    "\n",
    "                analytics_data.append({\n",
    "                    'timestamp': time.time(),\n",
    "                    'total_events': total_events,\n",
    "                    'sources_active': len(source_rows),\n",
    "                    'business_functions': len([r for r in function_rows if r.business_function != \"Other\"]),\n",
    "                    'recent_events': sum(row.recent_count for row in recent_rows) if recent_rows else 0\n",
    "                })\n",
    "                \n",
    "                if len(analytics_data) >= 2:\n",
    "                    latest = analytics_data[-1]\n",
    "                    previous = analytics_data[-2]\n",
    "                    trend = latest['total_events'] - previous['total_events']\n",
    "                    recent_trend = latest['recent_events'] - previous['recent_events']\n",
    "                    \n",
    "                    print(f\"\\nüìà TREND ANALYSIS:\")\n",
    "                    print(f\"   Total events trend: {'+' if trend > 0 else ''}{trend} events vs previous check\")\n",
    "                    print(f\"   Recent activity trend: {'+' if recent_trend > 0 else ''}{recent_trend} events\")\n",
    "                    print(f\"   Active sources: {latest['sources_active']}\")\n",
    "                    print(f\"   Business functions: {latest['business_functions']}\")\n",
    "                    \n",
    "                    if latest['business_functions'] >= 3:\n",
    "                        print(f\"   üü¢ Healthy: Multiple business functions active\")\n",
    "                    elif latest['business_functions'] >= 2:\n",
    "                        print(f\"   üü° Moderate: Some business functions active\")\n",
    "                    else:\n",
    "                        print(f\"   üü† Limited: Few business functions active\")\n",
    "                \n",
    "                sample_events = current_data.select(\"event_source\", \"event_time\", \"message_size\", \"priority\") \\\n",
    "                    .orderBy(F.desc(\"event_time\")) \\\n",
    "                    .limit(3)\n",
    "                \n",
    "                sample_rows = sample_events.collect()\n",
    "                if sample_rows:\n",
    "                    print(f\"\\nüîç SAMPLE RECENT EVENTS:\")\n",
    "                    for row in sample_rows:\n",
    "                        print(f\"   {row.event_source}: {row.message_size}B, {row.priority} priority\")\n",
    "            \n",
    "            else:\n",
    "                print(\"‚è≥ No events processed yet. Waiting for data flow...\")\n",
    "                print(\"üí° Ensure Data Forge is generating events:\")\n",
    "                print(\"   docker compose --profile datagen up -d\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Analytics update {i+1}: {e}\")\n",
    "            \n",
    "        time.sleep(2)\n",
    "    \n",
    "    business_analytics.stop()\n",
    "    print(f\"\\nüõë Production business analytics demonstration complete\")\n",
    "    \n",
    "    print(f\"\\nüéì PRODUCTION VALUE DEMONSTRATED:\")\n",
    "    print(f\"   üìä Real-time business function monitoring across all events\")\n",
    "    print(f\"   ‚ö° System-wide event velocity tracking with schema validation\")\n",
    "    print(f\"   üìà Cross-functional trend analysis using DataFrame operations\")\n",
    "    print(f\"   üéØ Unified view of business operations with real Schema Registry integration\")\n",
    "    print(f\"   üí° Foundation for advanced correlation analytics without Avro complexity\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No unified stream available for business analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fab6f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Lesson Summary & Production Implementation\n",
    "\n",
    "**Congratulations!** You've mastered production-grade multi-topic streaming with the **PROVEN approach**.\n",
    "\n",
    "### \udfc6 Proven Patterns Implemented\n",
    "\n",
    "1. **Schema Metadata Extraction** ‚Üí Extract schema IDs from Confluent format without complex deserialization\n",
    "2. **DataFrame Operations** ‚Üí Use PySpark DataFrame functions instead of SQL for robust streaming\n",
    "3. **Real Schema Registry Integration** ‚Üí Validate events against actual registered schemas\n",
    "4. **Unified Architecture** ‚Üí Single stream, single checkpoint, simplified operations\n",
    "5. **Cross-Event Intelligence** ‚Üí Business correlation without Avro compatibility issues\n",
    "\n",
    "### üéì What You Mastered\n",
    "\n",
    "1. **Real Multi-Topic Subscription** ‚Üí Process ALL business events in unified stream\n",
    "2. **Production Schema Validation** ‚Üí Extract and validate schema IDs from real Confluent events\n",
    "3. **Cross-Event Analytics** ‚Üí Correlate business activities across topics in real-time\n",
    "4. **Streaming Best Practices** ‚Üí DataFrame operations, proper checkpointing, error handling\n",
    "5. **Business Intelligence** ‚Üí Real-time monitoring of complete business operations\n",
    "\n",
    "### üè≠ Production Architecture Benefits\n",
    "\n",
    "**The proven approach provides:**\n",
    "\n",
    "- ‚úÖ **Simplified Complexity** ‚Üí Schema metadata instead of full Avro deserialization\n",
    "- ‚úÖ **Robust Analytics** ‚Üí DataFrame operations handle streaming edge cases better than SQL\n",
    "- ‚úÖ **Real Schema Validation** ‚Üí Actual Schema Registry integration for production quality\n",
    "- ‚úÖ **Operational Efficiency** ‚Üí Single checkpoint, unified monitoring, easier troubleshooting\n",
    "- ‚úÖ **Compatibility Resilience** ‚Üí Avoids Spark-Avro version conflicts in production\n",
    "\n",
    "### üöÄ Advanced Production Patterns\n",
    "\n",
    "**Scale this approach for enterprise:**\n",
    "\n",
    "```python\n",
    "# Customer journey correlation with schema validation\n",
    "user_journey_analysis = unified_stream.select(\n",
    "    F.col(\"user_id\"),\n",
    "    F.col(\"event_source\"), \n",
    "    F.col(\"schema_type\"),\n",
    "    F.col(\"event_time\")\n",
    ").filter(F.col(\"schema_type\") != \"unknown\") \\\n",
    " .groupBy(\"user_id\") \\\n",
    " .agg(\n",
    "     F.collect_list(\"event_source\").alias(\"journey\"),\n",
    "     F.count(\"*\").alias(\"touchpoints\"),\n",
    "     F.max(\"event_time\").alias(\"last_activity\")\n",
    " )\n",
    "\n",
    "# Real-time business process monitoring\n",
    "process_health = unified_stream.select(\n",
    "    F.when(F.col(\"event_source\").contains(\"orders\"), F.lit(\"Order Management\"))\n",
    "     .when(F.col(\"event_source\").contains(\"payments\"), F.lit(\"Payment Processing\"))\n",
    "     .alias(\"business_process\"),\n",
    "    F.col(\"schema_id\"),\n",
    "    F.col(\"schema_type\")\n",
    ").filter(F.col(\"schema_type\") != \"unknown\") \\\n",
    " .groupBy(\"business_process\") \\\n",
    " .agg(F.count(\"*\").alias(\"healthy_events\"))\n",
    "```\n",
    "\n",
    "### üîß Production Deployment Checklist\n",
    "\n",
    "**Before deploying this pattern:**\n",
    "\n",
    "- ‚úÖ Schema Registry connectivity and authentication configured\n",
    "- ‚úÖ Kafka consumer group management strategy defined  \n",
    "- ‚úÖ Checkpoint location on persistent, distributed storage\n",
    "- ‚úÖ Monitoring and alerting on schema validation failures\n",
    "- ‚úÖ Error handling and dead letter queue setup\n",
    "- ‚úÖ Resource allocation for sustained throughput\n",
    "- ‚úÖ Security and access control for all data sources\n",
    "\n",
    "### üí° Enterprise Extensions\n",
    "\n",
    "**Build on this foundation:**\n",
    "\n",
    "- **Event-Driven Microservices** ‚Üí Use correlation patterns for service choreography\n",
    "- **Real-Time ML Features** ‚Üí Extract features from validated event schemas\n",
    "- **Business Process Mining** ‚Üí Analyze end-to-end process flows across events\n",
    "- **Compliance Monitoring** ‚Üí Track data lineage through schema validation\n",
    "- **Operational Intelligence** ‚Üí Monitor system health through event patterns\n",
    "\n",
    "### üéñÔ∏è Key Success Factors\n",
    "\n",
    "**Why this approach works in production:**\n",
    "\n",
    "1. **Schema Metadata Strategy** ‚Üí Balances validation with performance\n",
    "2. **DataFrame-First Design** ‚Üí Robust against streaming edge cases  \n",
    "3. **Real Data Integration** ‚Üí Proves approach works with actual business events\n",
    "4. **Unified Processing Model** ‚Üí Simplifies architecture and operations\n",
    "5. **Incremental Complexity** ‚Üí Start simple, add sophistication gradually\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßπ MULTI-TOPIC LESSON CLEANUP:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "active_queries = spark.streams.active\n",
    "if active_queries:\n",
    "    print(f\"üõë Stopping {len(active_queries)} active streaming queries...\")\n",
    "    for query in active_queries:\n",
    "        try:\n",
    "            query.stop()\n",
    "            print(f\"   ‚úÖ Stopped: {query.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error stopping {query.name}: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No active queries to stop\")\n",
    "\n",
    "try:\n",
    "    import shutil\n",
    "    checkpoint_paths = [\n",
    "        \"/tmp/production-multi-topic-checkpoint\"\n",
    "    ]\n",
    "    \n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            shutil.rmtree(checkpoint_path)\n",
    "            print(f\"üóëÔ∏è Cleaned up checkpoint: {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Checkpoint cleanup: {e}\")\n",
    "\n",
    "try:\n",
    "    memory_tables = [\n",
    "        \"production_event_analytics\",\n",
    "        \"cross_event_correlation\", \n",
    "        \"production_business_analytics\",\n",
    "        \"event_source_analytics\",\n",
    "        \"correlation_demo\",\n",
    "        \"business_analytics_schema\"\n",
    "    ]\n",
    "    \n",
    "    for table in memory_tables:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            print(f\"üóëÔ∏è Cleared memory table: {table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Could not clear {table}: {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Memory table cleanup: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Production lesson cleanup complete!\")\n",
    "print(f\"üí° Spark session remains active for further experimentation\")\n",
    "print(f\"\\nüéâ LESSON COMPLETION SUMMARY:\")\n",
    "print(f\"   üìä Processed real Kafka events with Schema Registry validation\")\n",
    "print(f\"   üåä Implemented unified multi-topic streaming architecture\")\n",
    "print(f\"   ‚ö° Used DataFrame operations for robust streaming analytics\")\n",
    "print(f\"   üîó Demonstrated cross-event correlation without Avro complexity\")\n",
    "print(f\"   üéØ Ready for production multi-topic streaming deployment!\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   üìö Explore advanced Data Forge lessons\")\n",
    "print(f\"   üè≠ Deploy this pattern in production environments\")\n",
    "print(f\"   üîß Extend with custom business logic and correlation rules\")\n",
    "print(f\"   üìà Build real-time dashboards using these analytics patterns\")\n",
    "\n",
    "# Optional: Uncomment to stop Spark completely\n",
    "# spark.stop()\n",
    "# print(\"üèÅ Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286adc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
