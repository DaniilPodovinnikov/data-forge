{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfa390f",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Building a Raw Events (Bronze) Layer with Iceberg\n",
    "\n",
    "This lesson demonstrates how to build a robust, queryable raw events layer (bronze layer) using Apache Iceberg.\n",
    "\n",
    "You'll learn how to aggregate real-time events from multiple Kafka topics, validate schemas with Schema Registry, and write the unified stream to an Iceberg table.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Iceberg for the Raw Layer?\n",
    "- **ACID transactions**: Reliable ingestion, even with late or duplicate events.\n",
    "- **Schema evolution**: Supports changing event structures over time.\n",
    "- **Time travel**: Enables auditing and replay.\n",
    "- **Separation of storage and compute**: Efficient for large-scale, multi-tenant analytics.\n",
    "\n",
    "The raw (bronze) layer is the foundation for all downstream analytics. It must be reliable, auditable, and flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57670e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Environment Setup\n",
    "\n",
    "Ensure all core services are running, including Kafka, Schema Registry, MinIO, Hive Metastore, and Spark.\n",
    "\n",
    "```bash\n",
    "docker compose --profile core --profile datagen up -d\n",
    "```\n",
    "\n",
    "This lesson assumes the data generator is producing events to all retail topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0467b884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T12:56:25.524641Z",
     "iopub.status.busy": "2025-09-11T12:56:25.524276Z",
     "iopub.status.idle": "2025-09-11T12:56:25.532113Z",
     "shell.execute_reply": "2025-09-11T12:56:25.531599Z",
     "shell.execute_reply.started": "2025-09-11T12:56:25.524620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment configured for AWS region us-east-1\n",
      "MinIO: http://minio:9000\n",
      "Hive Metastore: thrift://hive-metastore:9083\n",
      "Spark Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ROOT_USER', 'minio')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_ROOT_PASSWORD', 'minio123')\n",
    "HIVE_METASTORE_URI = \"thrift://hive-metastore:9083\"\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "RETAIL_TOPICS = \"orders.v1,payments.v1,shipments.v1,inventory-changes.v1,customer-interactions.v1\"\n",
    "S3_ENDPOINT = \"minio:9000\"\n",
    "S3_ACCESS_KEY = MINIO_ACCESS_KEY\n",
    "S3_SECRET_KEY = MINIO_SECRET_KEY\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "print(\"üîß Environment configured for AWS region us-east-1\")\n",
    "print(f\"MinIO: {MINIO_ENDPOINT}\")\n",
    "print(f\"Hive Metastore: {HIVE_METASTORE_URI}\")\n",
    "print(f\"Spark Master: {SPARK_MASTER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10668ff8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì® Read and Validate Kafka Events\n",
    "\n",
    "Read all retail topics as a single stream, extract schema IDs, and validate with Schema Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4987290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T12:56:28.395778Z",
     "iopub.status.busy": "2025-09-11T12:56:28.395322Z",
     "iopub.status.idle": "2025-09-11T12:56:34.194697Z",
     "shell.execute_reply": "2025-09-11T12:56:34.194000Z",
     "shell.execute_reply.started": "2025-09-11T12:56:28.395759Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session created successfully with robust AWS region config\n",
      "üìä Spark UI available at: http://localhost:8088\n",
      "üîß Spark version: 3.5.0\n",
      "üåê Iceberg REST Catalog: http://hive-metastore:9001/iceberg\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = [\n",
    "    'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2',\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.4',\n",
    "    'software.amazon.awssdk:bundle:2.20.158',\n",
    "    'org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0',\n",
    "    'org.apache.spark:spark-avro_2.12:3.4.0',\n",
    "    'org.apache.avro:avro:1.11.0'\n",
    " ]\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"IcebergBronzeDemo\") \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"spark.executor.memory\", \"512m\") \\\n",
    "        .config(\"spark.driver.memory\", \"512m\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.cores.max\", \"2\") \\\n",
    "        .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.type\", \"rest\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.uri\", \"http://hive-metastore:9001/iceberg\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://iceberg/warehouse\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.endpoint\", f\"http://{S3_ENDPOINT}\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", S3_ACCESS_KEY) \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", S3_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"us-east-1\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.region\", \"us-east-1\") \\\n",
    "        .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\") \\\n",
    "        .config(\"spark.executorEnv.AWS_DEFAULT_REGION\", \"us-east-1\") \\\n",
    "        .config(\"spark.executorEnv.aws.region\", \"us-east-1\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Daws.region=us-east-1\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Daws.region=us-east-1\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "        .config(\"spark.sql.defaultCatalog\", \"iceberg\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", HIVE_METASTORE_URI) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    print(\"‚úÖ Spark session created successfully with robust AWS region config\")\n",
    "    print(f\"üìä Spark UI available at: http://localhost:8088\")\n",
    "    print(f\"üîß Spark version: {spark.version}\")\n",
    "    print(f\"üåê Iceberg REST Catalog: http://hive-metastore:9001/iceberg\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå All attempts failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949b7c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Create the Iceberg Bronze Table\n",
    "\n",
    "We create a single Iceberg table to store all raw events from all Kafka topics. This table is designed for flexibility, auditability, and efficient downstream analytics.\n",
    "\n",
    "**Key design choices:**\n",
    "- **Wide, unified schema:** All event types are stored together, with a flexible payload column for the decoded JSON event.\n",
    "- **Partitioned by event_source:** Enables efficient queries by event type and supports scalable ingestion.\n",
    "- **ACID guarantees:** Iceberg ensures reliable, atomic streaming writes, even with late or duplicate events.\n",
    "- **Schema evolution:** The table can evolve as event schemas change, with full history and time travel.\n",
    "\n",
    "**DDL Example:**\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS iceberg.bronze_example.raw_events (\n",
    "  event_source STRING,         -- Kafka topic name (event type)\n",
    "  event_time TIMESTAMP,        -- Event timestamp from Kafka\n",
    "  schema_id INT,               -- Avro schema ID from Schema Registry\n",
    "  payload_size INT,            -- Size of the Avro payload\n",
    "  json_payload STRING,         -- Decoded event as JSON (for easy analytics)\n",
    "  partition INT,               -- Kafka partition\n",
    "  offset BIGINT                -- Kafka offset\n",
    " )\n",
    "USING iceberg\n",
    "PARTITIONED BY (event_source)\n",
    "TBLPROPERTIES ('write.format.default'='parquet')\n",
    "```\n",
    "\n",
    "This structure enables fast, flexible analytics and robust data management for all downstream use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ca85cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T12:56:38.188582Z",
     "iopub.status.busy": "2025-09-11T12:56:38.187606Z",
     "iopub.status.idle": "2025-09-11T12:56:40.374777Z",
     "shell.execute_reply": "2025-09-11T12:56:40.373944Z",
     "shell.execute_reply.started": "2025-09-11T12:56:38.188545Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg bronze table ready.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS iceberg.bronze_example\n",
    "\"\"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS iceberg.bronze_example.raw_events\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg.bronze_example.raw_events (\n",
    "  event_source STRING,\n",
    "  event_time TIMESTAMP,\n",
    "  schema_id INT,\n",
    "  payload_size INT,\n",
    "  json_payload STRING,\n",
    "  partition INT,\n",
    "  offset BIGINT\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (event_source)\n",
    "TBLPROPERTIES ('write.format.default'='parquet')\n",
    "\"\"\")\n",
    "print('Iceberg bronze table ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b412249",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Write the Unified Stream to Iceberg\n",
    "\n",
    "Write all validated raw events to the Iceberg bronze table. This enables downstream analytics, audit, and replay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd52132",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Note on Bronze Layer Design\n",
    "\n",
    "**Production best practice:**  \n",
    "The Bronze layer should store a byte-for-byte copy of what arrives from Kafka, with no transformation or interpretation.\n",
    "\n",
    "- **Reproducibility:** Enables re-processing and rebuilding downstream layers if decoding bugs or schema changes occur.\n",
    "- **Auditability:** Guarantees a verifiable record of exactly what was received.\n",
    "- **Flexibility:** Allows future re-decoding for different use cases (analytics, ML, etc.).\n",
    "\n",
    "**Demo note:**  \n",
    "For learning and demo purposes, this notebook decodes Avro payloads to JSON immediately.  \n",
    "- **Why?** JSON is easy to inspect and query with tools like Trino or Superset, making workshops and demos more accessible.\n",
    "\n",
    "**Summary:**  \n",
    "- **Production:** Bronze = immutable raw bytes + metadata.  \n",
    "- **This demo:** Bronze-JSON = easy-to-query teaching aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d082b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T12:56:43.130962Z",
     "iopub.status.busy": "2025-09-11T12:56:43.130618Z",
     "iopub.status.idle": "2025-09-11T13:05:26.578528Z",
     "shell.execute_reply": "2025-09-11T13:05:26.575766Z",
     "shell.execute_reply.started": "2025-09-11T12:56:43.130939Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified raw events stream (with JSON payload) is writing to Iceberg bronze layer.\n",
      "Batch 1: 0 events in Iceberg bronze table\n",
      "Empty DataFrame\n",
      "Columns: [json_payload]\n",
      "Index: []\n",
      "Batch 2: 169 events in Iceberg bronze table\n",
      "                                        json_payload\n",
      "0  {\"order_id\": \"ord_zi1xsmoms6\", \"user_id\": \"U00...\n",
      "1  {\"order_id\": \"ord_gk4xqf6d10\", \"user_id\": \"U00...\n",
      "2  {\"order_id\": \"ord_5skcjfp1we\", \"user_id\": \"U00...\n",
      "Batch 3: 169 events in Iceberg bronze table\n",
      "                                        json_payload\n",
      "0  {\"order_id\": \"ord_zi1xsmoms6\", \"user_id\": \"U00...\n",
      "1  {\"order_id\": \"ord_gk4xqf6d10\", \"user_id\": \"U00...\n",
      "2  {\"order_id\": \"ord_5skcjfp1we\", \"user_id\": \"U00...\n",
      "Batch 4: 1163 events in Iceberg bronze table\n",
      "                                        json_payload\n",
      "0  {\"order_id\": \"ord_aht1wtyy0r\", \"user_id\": \"U00...\n",
      "1  {\"order_id\": \"ord_y29zzug4ou\", \"user_id\": \"U00...\n",
      "2  {\"order_id\": \"ord_4frhvuc4te\", \"user_id\": \"U00...\n",
      "Batch 5: 2881 events in Iceberg bronze table\n",
      "                                        json_payload\n",
      "0  {\"order_id\": \"ord_zi1xsmoms6\", \"user_id\": \"U00...\n",
      "1  {\"order_id\": \"ord_gk4xqf6d10\", \"user_id\": \"U00...\n",
      "2  {\"order_id\": \"ord_5skcjfp1we\", \"user_id\": \"U00...\n",
      "Bronze layer streaming ingestion stopped.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import io\n",
    "import avro.schema\n",
    "import avro.io\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def decode_avro_message(value, schema_registry_url):\n",
    "    if value is None or len(value) < 5:\n",
    "        return json.dumps({'error': 'invalid or missing value'})\n",
    "    magic_byte = value[0]\n",
    "    schema_id = int.from_bytes(value[1:5], byteorder='big')\n",
    "    avro_payload = value[5:]\n",
    "    if magic_byte != 0:\n",
    "        return json.dumps({'error': f'invalid magic byte: {magic_byte}', 'schema_id': schema_id})\n",
    "    try:\n",
    "        sr_client = SchemaRegistryClient({'url': schema_registry_url})\n",
    "        schema = sr_client.get_schema(schema_id)\n",
    "        avro_schema = avro.schema.parse(schema.schema_str)\n",
    "        bytes_reader = io.BytesIO(avro_payload)\n",
    "        decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "        reader = avro.io.DatumReader(avro_schema)\n",
    "        decoded = reader.read(decoder)\n",
    "        def default(obj):\n",
    "            if isinstance(obj, (datetime.datetime, datetime.date)):\n",
    "                return obj.isoformat()\n",
    "            raise TypeError\n",
    "        return json.dumps(decoded, default=default)\n",
    "    except Exception as e:\n",
    "        return json.dumps({'error': f'avro decode error: {str(e)}', 'schema_id': schema_id})\n",
    "\n",
    "SCHEMA_REGISTRY_URL = os.environ.get('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "\n",
    "decode_udf = udf(lambda v: decode_avro_message(v, SCHEMA_REGISTRY_URL), StringType())\n",
    "\n",
    "raw_stream = (\n",
    "    spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "        .option(\"subscribe\", RETAIL_TOPICS)\n",
    "        .option(\"startingOffsets\", \"latest\")\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "        .option(\"kafka.consumer.group.id\", \"bronze-layer-consumer\")\n",
    "        .load()\n",
    "        .select(\n",
    "            F.col(\"topic\").alias(\"event_source\"),\n",
    "            F.col(\"timestamp\").alias(\"event_time\"),\n",
    "            F.col(\"partition\"),\n",
    "            F.col(\"offset\"),\n",
    "            F.col(\"value\"),\n",
    "            F.when(F.length(\"value\") >= 5, F.conv(F.hex(F.substring(\"value\", 2, 4)), 16, 10).cast(\"int\")).alias(\"schema_id\"),\n",
    "            F.when(F.length(\"value\") > 5, F.length(\"value\") - 5).otherwise(0).alias(\"payload_size\")\n",
    "        )\n",
    ")\n",
    "\n",
    "raw_stream = raw_stream.withColumn('json_payload', decode_udf(\"value\"))\n",
    "\n",
    "bronze_query = (\n",
    "    raw_stream\n",
    "    .select('event_source', 'event_time', 'schema_id', 'payload_size', 'json_payload', 'partition', 'offset')\n",
    "    .writeStream\n",
    "    .format('iceberg')\n",
    "    .outputMode('append')\n",
    "    .option('checkpointLocation', '/tmp/bronze-layer-checkpoint')\n",
    "    .option('path', 'iceberg.bronze_example.raw_events')\n",
    "    .trigger(processingTime='5 seconds')\n",
    "    .start()\n",
    ")\n",
    "print('Unified raw events stream (with JSON payload) is writing to Iceberg bronze layer.')\n",
    "\n",
    "import time\n",
    "for i in range(5):\n",
    "    time.sleep(5)\n",
    "    df = spark.read.format('iceberg').load('iceberg.bronze_example.raw_events')\n",
    "    print(f\"Batch {i+1}: {df.count()} events in Iceberg bronze table\")\n",
    "    print(df.select('json_payload').limit(3).toPandas())\n",
    "\n",
    "bronze_query.stop()\n",
    "print('Bronze layer streaming ingestion stopped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e5270",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîé Query and Audit the Bronze Layer\n",
    "\n",
    "You can now query the raw events table for any event type, time window, or schema version.\n",
    "\n",
    "Example: Count events by source and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07778ef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-11T13:05:49.355549Z",
     "iopub.status.busy": "2025-09-11T13:05:49.352909Z",
     "iopub.status.idle": "2025-09-11T13:05:51.296272Z",
     "shell.execute_reply": "2025-09-11T13:05:51.295712Z",
     "shell.execute_reply.started": "2025-09-11T13:05:49.355474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+\n",
      "|        event_source|      date|count|\n",
      "+--------------------+----------+-----+\n",
      "|customer-interact...|2025-09-11| 1000|\n",
      "|           orders.v1|2025-09-11|  752|\n",
      "|        shipments.v1|2025-09-11|  195|\n",
      "|inventory-changes.v1|2025-09-11|  429|\n",
      "|         payments.v1|2025-09-11|  505|\n",
      "+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = spark.read.format('iceberg').load('iceberg.bronze_example.raw_events')\n",
    "df.groupBy('event_source', F.to_date('event_time').alias('date')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ BRONZE LAYER CLEANUP:\n",
      "==================================================\n",
      "‚ÑπÔ∏è No active queries to stop\n",
      "üóëÔ∏è Cleaned up checkpoint: /tmp/bronze-layer-checkpoint\n",
      "üóëÔ∏è Dropped Iceberg table: iceberg.bronze_example.raw_events\n",
      "üóëÔ∏è Dropped Iceberg database: iceberg.bronze_example\n",
      "üóëÔ∏è Dropped Iceberg database: iceberg.bronze_example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 46110)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"üßπ BRONZE LAYER CLEANUP:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "active_queries = spark.streams.active\n",
    "if active_queries:\n",
    "    print(f\"üõë Stopping {len(active_queries)} active streaming queries...\")\n",
    "    for query in active_queries:\n",
    "        try:\n",
    "            query.stop()\n",
    "            print(f\"   ‚úÖ Stopped: {query.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error stopping {query.name}: {e}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No active queries to stop\")\n",
    "\n",
    "try:\n",
    "    import shutil\n",
    "    checkpoint_paths = [\n",
    "        \"/tmp/bronze-layer-checkpoint\"\n",
    "    ]\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            shutil.rmtree(checkpoint_path)\n",
    "            print(f\"üóëÔ∏è Cleaned up checkpoint: {checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Checkpoint cleanup: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS iceberg.bronze_example.raw_events\")\n",
    "    print(\"üóëÔ∏è Dropped Iceberg table: iceberg.bronze_example.raw_events\")\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS iceberg.bronze_example\")\n",
    "    print(\"üóëÔ∏è Dropped Iceberg database: iceberg.bronze_example\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Iceberg cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e9667",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Iceberg Time Travel & Table Optimization\n",
    "\n",
    "Apache Iceberg enables powerful features for data reliability and analytics, including time travel and table optimization.\n",
    "\n",
    "### Time Travel: Querying Historical Snapshots\n",
    "\n",
    "You can query your raw events table as it existed at a previous point in time or at a specific snapshot. This is useful for:\n",
    "\n",
    "- **Auditing:** See exactly what data was present at a given time.\n",
    "- **Reproducibility:** Re-run analytics on historical data states.\n",
    "- **Debugging:** Investigate the impact of late or erroneous events.\n",
    "\n",
    "**Example: Query by Snapshot ID**\n",
    "```python\n",
    "# Get the latest snapshot ID\n",
    "snapshots = spark.sql(\"SELECT * FROM iceberg.bronze_example.raw_events.snapshots\")\n",
    "snapshots.select(\"snapshot_id\", \"committed_at\").show()\n",
    "\n",
    "# Query the table as of a specific snapshot\n",
    "snapshot_id = snapshots.orderBy(\"committed_at\", ascending=False).first()[\"snapshot_id\"]\n",
    "df_snapshot = spark.read.option(\"snapshot-id\", snapshot_id).format(\"iceberg\").load(\"iceberg.bronze_example.raw_events\")\n",
    "df_snapshot.show(5)\n",
    "```\n",
    "\n",
    "**Example: Query by Timestamp**\n",
    "```python\n",
    "# Query the table as of a specific timestamp (in ms since epoch)\n",
    "import time\n",
    "ts = int(time.time() * 1000)  # current time, replace with your desired timestamp\n",
    "df_asof = spark.read.option(\"as-of-timestamp\", ts).format(\"iceberg\").load(\"iceberg.bronze_example.raw_events\")\n",
    "df_asof.show(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Table Optimization: Remove Old Snapshots and Expired Data Files\n",
    "\n",
    "Over time, streaming ingestion creates many snapshots and orphaned files. Regular cleanup is recommended for performance and cost.\n",
    "\n",
    "**Expire Snapshots (Keep Last 2 Days):**\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    CALL iceberg.system.expire_snapshots(\n",
    "        table => 'iceberg.bronze_example.raw_events',\n",
    "        older_than => TIMESTAMPADD('DAY', -2, CURRENT_TIMESTAMP),\n",
    "        retain_last => 1\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Remove Orphan Files:**\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    CALL iceberg.system.remove_orphan_files(\n",
    "        table => 'iceberg.bronze_example.raw_events'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Rewrite Manifests and Data Files (Compaction):**\n",
    "```python\n",
    "spark.sql(\"\"\"\n",
    "    CALL iceberg.system.rewrite_manifests('iceberg.bronze_example.raw_events')\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    CALL iceberg.system.rewrite_data_files('iceberg.bronze_example.raw_events')\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46862ee2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes & Best Practices\n",
    "\n",
    "- **Partitioning**: Partition by event source and date for efficient queries.\n",
    "- **Schema evolution**: Iceberg supports adding/removing fields as event schemas change.\n",
    "- **Auditability**: All raw events are stored with schema ID and timestamp for full traceability.\n",
    "- **Downstream processing**: Silver/gold layers can read from this bronze table for further enrichment and analytics.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- Iceberg provides a robust, flexible foundation for the raw events layer.\n",
    "- Unified streaming ingestion enables audit, replay, and schema validation.\n",
    "- This pattern is production-ready and extensible for any event-driven architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
