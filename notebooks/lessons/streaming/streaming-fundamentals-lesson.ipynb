{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b71d4bb",
   "metadata": {},
   "source": [
    "# 🌊 Streaming Fundamentals: A Data Forge Lesson\n",
    "\n",
    "**Learn real-time data streaming with Apache Spark, Kafka, and Avro**\n",
    "\n",
    "This lesson demonstrates core streaming concepts using Data Forge's retail data generator. You'll understand why Avro beats JSON, how Schema Registry enables evolution, and how PySpark Structured Streaming works.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this lesson, you'll understand:\n",
    "\n",
    "1. **Prerequisites** → How to start Data Forge's data generator\n",
    "2. **Data Examination** → What streaming retail data looks like\n",
    "3. **Avro vs JSON** → Why Avro is superior for streaming (with evidence)\n",
    "4. **Schema Registry** → How it enables safe schema evolution\n",
    "5. **PySpark Streaming** → How Structured Streaming processes infinite data\n",
    "6. **Checkpoints** → Why they're critical for fault tolerance\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Prerequisites\n",
    "\n",
    "**Before starting this lesson:**\n",
    "\n",
    "```bash\n",
    "# 1. Start Data Forge core services\n",
    "docker compose --profile core up -d\n",
    "\n",
    "# 2. Start the data generator\n",
    "docker compose --profile datagen up -d\n",
    "\n",
    "# 3. Verify data is flowing\n",
    "docker compose logs -f data-generator | head -20\n",
    "```\n",
    "\n",
    "🛑 **Without the data generator, this lesson won't work.** The generator produces the streaming events we'll analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718a7c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚙️ Setup & Configuration\n",
    "\n",
    "Initialize our streaming environment and validate connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43613fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Data Forge service endpoints\n",
    "KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "\n",
    "print(\"🔥 Data Forge Streaming Configuration:\")\n",
    "print(f\"   Kafka Bootstrap: {KAFKA_BOOTSTRAP}\")\n",
    "print(f\"   Schema Registry: {SCHEMA_REGISTRY_URL}\")\n",
    "print(f\"   Spark Master: {SPARK_MASTER}\")\n",
    "print(f\"   Lesson Start: {datetime.now()}\")\n",
    "print(\"\\n✅ Ready to learn streaming fundamentals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb651d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔍 Lesson 1: Examining Streaming Data\n",
    "\n",
    "**Goal:** Understand what real-time retail data looks like.\n",
    "\n",
    "Data Forge's generator produces realistic business events: orders, payments, shipments, inventory changes, and customer interactions. Let's examine this data to understand streaming patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer\n",
    "import json\n",
    "\n",
    "kafka_config = {\n",
    "    'bootstrap.servers': 'kafka:9092',\n",
    "    'group.id': 'streaming-lesson-' + str(hash('lesson') % 10000),\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "\n",
    "def sample_streaming_data(topic_name, num_messages=2):\n",
    "    \"\"\"Sample live messages from a Kafka topic to understand data structure\"\"\"\n",
    "    print(f\"📊 Sampling {num_messages} live messages from {topic_name}...\")\n",
    "    \n",
    "    consumer = Consumer(kafka_config)\n",
    "    \n",
    "    try:\n",
    "        consumer.subscribe([topic_name])\n",
    "        messages = []\n",
    "        attempts = 0\n",
    "        max_attempts = 30\n",
    "        \n",
    "        while len(messages) < num_messages and attempts < max_attempts:\n",
    "            msg = consumer.poll(timeout=1.0)\n",
    "            attempts += 1\n",
    "            \n",
    "            if attempts % 10 == 0:\n",
    "                print(f\"   🔄 Polling attempt {attempts}/{max_attempts}...\")\n",
    "            \n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                print(f\"❌ Consumer error: {msg.error()}\")\n",
    "                continue\n",
    "                \n",
    "            messages.append(msg)\n",
    "            \n",
    "            print(f\"\\n✅ MESSAGE {len(messages)} CAPTURED:\")\n",
    "            print(f\"   📍 Topic: {msg.topic()}, Partition: {msg.partition()}, Offset: {msg.offset()}\")\n",
    "            print(f\"   ⏰ Timestamp: {msg.timestamp()}\")\n",
    "            key = msg.key()\n",
    "            if key:\n",
    "                try:\n",
    "                    key_str = key.decode('utf-8') if isinstance(key, bytes) else str(key)\n",
    "                    print(f\"   🔑 Key: {key_str}\")\n",
    "                except:\n",
    "                    print(f\"   🔑 Key: {key} (binary)\")\n",
    "            else:\n",
    "                print(f\"   🔑 Key: None\")\n",
    "            value = msg.value()\n",
    "            if value:\n",
    "                print(f\"   📦 Value length: {len(value)} bytes\")\n",
    "                hex_preview = ' '.join([f'{b:02x}' for b in value[:20]])\n",
    "                print(f\"   🔍 Hex preview: {hex_preview}...\")\n",
    "                if len(value) >= 5 and value[0] == 0:\n",
    "                    schema_id = int.from_bytes(value[1:5], byteorder='big')\n",
    "                    print(f\"   📋 Avro schema ID: {schema_id}\")\n",
    "                    print(f\"   ✅ Confluent Schema Registry format detected\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ Not standard Confluent Avro format\")\n",
    "            else:\n",
    "                print(f\"   📦 Value: None\")\n",
    "        \n",
    "        if len(messages) == 0:\n",
    "            print(\"🛑 No messages found - check if data generator is running!\")\n",
    "        else:\n",
    "            print(f\"\\n🎯 Successfully sampled {len(messages)} messages from {topic_name}\")\n",
    "            \n",
    "        return messages\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "print(\"🔗 Testing Kafka connectivity...\")\n",
    "try:\n",
    "    consumer = Consumer({'bootstrap.servers': 'kafka:9092', 'group.id': 'test-connectivity'})\n",
    "    metadata = consumer.list_topics(timeout=10)\n",
    "    print(f\"✅ Connected to Kafka. Found {len(metadata.topics)} topics:\")\n",
    "    \n",
    "    retail_topics = []\n",
    "    for topic_name in sorted(metadata.topics.keys()):\n",
    "        if not topic_name.startswith('_'):\n",
    "            partitions = len(metadata.topics[topic_name].partitions)\n",
    "            print(f\"   📊 {topic_name} ({partitions} partitions)\")\n",
    "            if topic_name.endswith('.v1'):\n",
    "                retail_topics.append(topic_name)\n",
    "    \n",
    "    print(f\"\\n🏪 Retail streaming topics: {retail_topics}\")\n",
    "    consumer.close()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Kafka connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🛒 EXAMINING ORDERS DATA:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "orders_messages = sample_streaming_data(\"orders.v1\", 2)\n",
    "\n",
    "print(\"\\n🎓 LESSON INSIGHT:\")\n",
    "print(\"Notice the binary data format - this is Avro, not JSON.\")\n",
    "print(\"The magic byte (00) + schema ID (4 bytes) tells us it's Confluent format.\")\n",
    "print(\"This compact binary encoding is why Avro beats JSON for streaming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f8a4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📋 Lesson 2: Schema Registry Deep Dive\n",
    "\n",
    "**Goal:** Understand how Schema Registry enables safe schema evolution.\n",
    "\n",
    "Schema Registry is like a \"contract database\" for your streaming data. It stores Avro schemas and enforces compatibility rules, preventing breaking changes that would crash your streaming pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "\n",
    "sr_client = SchemaRegistryClient({'url': 'http://schema-registry:8081'})\n",
    "\n",
    "print(\"📋 SCHEMA REGISTRY EXPLORATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    subjects = sr_client.get_subjects()\n",
    "    print(f\"📊 Total schema subjects: {len(subjects)}\")\n",
    "    print(\"\\n🔍 Available schemas:\")\n",
    "    \n",
    "    for subject in sorted(subjects):\n",
    "        try:\n",
    "            versions = sr_client.get_versions(subject)\n",
    "            latest_version = sr_client.get_latest_version(subject)\n",
    "            print(f\"   📋 {subject}: {len(versions)} versions (latest: v{latest_version.version}, schema ID: {latest_version.schema_id})\")\n",
    "            if subject == \"orders.v1-value\":\n",
    "                schema_str = latest_version.schema.schema_str\n",
    "                schema_obj = json.loads(schema_str)\n",
    "                \n",
    "                print(f\"\\n🛒 ORDERS SCHEMA BREAKDOWN:\")\n",
    "                print(f\"   📝 Schema name: {schema_obj['name']}\")\n",
    "                print(f\"   📊 Number of fields: {len(schema_obj['fields'])}\")\n",
    "                print(f\"   🔧 Fields:\")\n",
    "                for field in schema_obj['fields']:\n",
    "                    field_type = field['type']\n",
    "                    print(f\"      • {field['name']}: {field_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {subject}: (error getting version info: {e})\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Schema Registry error: {e}\")\n",
    "\n",
    "print(f\"\\n🎓 LESSON INSIGHT:\")\n",
    "print(f\"Schema Registry acts as a 'contract database' for streaming data.\")\n",
    "print(f\"Each message references a schema ID, enabling safe evolution without breaking consumers.\")\n",
    "print(f\"This is impossible with JSON - you'd need to parse every message to know its structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b017b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🆚 Lesson 3: Avro vs JSON - The Evidence\n",
    "\n",
    "**Goal:** Understand why Avro dominates streaming with concrete evidence.\n",
    "\n",
    "JSON seems simpler, but Avro wins on every metric that matters for streaming: size, speed, schema evolution, and type safety. Let's prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_avro_message(message_value):\n",
    "    \"\"\"Decode Avro message using proven working method\"\"\"\n",
    "    try:\n",
    "        if len(message_value) < 5:\n",
    "            return None\n",
    "            \n",
    "        magic_byte = message_value[0]\n",
    "        schema_id = int.from_bytes(message_value[1:5], byteorder='big')\n",
    "        avro_payload = message_value[5:]\n",
    "        \n",
    "        if magic_byte != 0:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            import io\n",
    "            import avro.schema\n",
    "            import avro.io\n",
    "            \n",
    "            schema = sr_client.get_schema(schema_id)\n",
    "            avro_schema = avro.schema.parse(schema.schema_str)\n",
    "            \n",
    "            bytes_reader = io.BytesIO(avro_payload)\n",
    "            decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "            reader = avro.io.DatumReader(avro_schema)\n",
    "            \n",
    "            decoded = reader.read(decoder)\n",
    "            return decoded\n",
    "            \n",
    "        except Exception:\n",
    "            return None\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"🆚 AVRO vs JSON COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if orders_messages:\n",
    "    sample_msg = orders_messages[0]\n",
    "    avro_data = decode_avro_message(sample_msg.value())\n",
    "    \n",
    "    if avro_data:\n",
    "        json_equivalent = json.dumps(avro_data, indent=2, default=str)\n",
    "        \n",
    "        avro_size = len(sample_msg.value())\n",
    "        json_size = len(json_equivalent.encode('utf-8'))\n",
    "        \n",
    "        print(f\"📊 SIZE COMPARISON:\")\n",
    "        print(f\"   🔹 Avro binary: {avro_size} bytes\")\n",
    "        print(f\"   🔹 JSON equivalent: {json_size} bytes\")\n",
    "        print(f\"   📈 Space savings: {((json_size - avro_size) / json_size * 100):.1f}% smaller with Avro\")\n",
    "        \n",
    "        print(f\"\\n📋 DECODED DATA:\")\n",
    "        for key, value in avro_data.items():\n",
    "            if isinstance(value, (int, float, bool)):\n",
    "                print(f\"   {key}: {value}\")\n",
    "            elif isinstance(value, str):\n",
    "                safe_value = ''.join(c if ord(c) < 128 else '?' for c in value)\n",
    "                print(f\"   {key}: '{safe_value}'\")\n",
    "            else:\n",
    "                print(f\"   {key}: {str(value)}\")\n",
    "        if 'ts' in avro_data:\n",
    "            try:\n",
    "                from datetime import datetime\n",
    "                ts_ms = avro_data['ts']\n",
    "                if isinstance(ts_ms, (int, float)):\n",
    "                    ts_readable = datetime.fromtimestamp(ts_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    print(f\"   ts_readable: {ts_readable}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        print(f\"\\n🎓 WHY AVRO WINS:\")\n",
    "        print(f\"   ✅ Size: {((json_size - avro_size) / json_size * 100):.1f}% smaller → less network/storage cost\")\n",
    "        print(f\"   ✅ Speed: Binary parsing is faster than JSON text parsing\")\n",
    "        print(f\"   ✅ Schema: Enforced types prevent runtime errors\")\n",
    "        print(f\"   ✅ Evolution: Add/remove fields without breaking consumers\")\n",
    "        print(f\"   ✅ Compression: Better compression ratios due to structure\")\n",
    "        \n",
    "        print(f\"\\n❌ JSON PROBLEMS:\")\n",
    "        print(f\"   ❌ No schema enforcement → runtime type errors\")\n",
    "        print(f\"   ❌ Text parsing overhead\")\n",
    "        print(f\"   ❌ Field name repetition in every message\")\n",
    "        print(f\"   ❌ No safe evolution strategy\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Could not decode Avro message\")\n",
    "else:\n",
    "    print(\"❌ No orders messages available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb869055",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⚡ Lesson 4: PySpark Structured Streaming Fundamentals\n",
    "\n",
    "**Goal:** Understand how Spark processes infinite data streams.\n",
    "\n",
    "Traditional batch processing reads finite data, processes it, and stops. Streaming is different - data never stops arriving. Spark Structured Streaming treats streams as \"unbounded tables\" that grow continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "print(\"⚡ SPARK STREAMING SETUP:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingFundamentalsLesson\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\"\n",
    "            \"org.apache.spark:spark-avro_2.12:3.4.0\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/streaming-lesson-checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"✅ Spark session created successfully\")\n",
    "print(f\"   📊 Spark version: {spark.version}\")\n",
    "print(f\"   🆔 Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"   🖥️ Master: {SPARK_MASTER}\")\n",
    "print(f\"   💾 Checkpoint location: /tmp/streaming-lesson-checkpoint\")\n",
    "print(f\"\\n🔗 Testing Spark-Kafka connectivity...\")\n",
    "try:\n",
    "    test_df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP) \\\n",
    "        .option(\"subscribe\", \"orders.v1\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "    \n",
    "    message_count = test_df.count()\n",
    "    print(f\"✅ Kafka connectivity test passed\")\n",
    "    print(f\"   📊 Found {message_count} messages in orders.v1 topic\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Kafka connectivity test failed: {e}\")\n",
    "\n",
    "print(f\"\\n🎓 STREAMING FUNDAMENTALS:\")\n",
    "print(f\"   • Streaming = processing unbounded (infinite) data\")\n",
    "print(f\"   • Spark treats streams as 'growing tables'\")\n",
    "print(f\"   • Each micro-batch processes new data incrementally\")\n",
    "print(f\"   • Checkpoints track progress for fault tolerance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beaf19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_streaming_dataframe(topic_name):\n",
    "    \"\"\"Create a streaming DataFrame - this represents infinite data\"\"\"\n",
    "    print(f\"🌊 Creating streaming DataFrame for {topic_name}\")\n",
    "    \n",
    "    try:\n",
    "        # This creates a streaming DataFrame - it represents infinite data\n",
    "        kafka_stream = (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "            .option(\"subscribe\", topic_name)\n",
    "            .option(\"startingOffsets\", \"latest\")  # Only process new data\n",
    "            .option(\"failOnDataLoss\", \"false\")    # Don't fail if data is lost\n",
    "            .load())\n",
    "        \n",
    "        # Transform the raw Kafka data into a structured format\n",
    "        structured_stream = kafka_stream.select(\n",
    "            col(\"key\").cast(\"string\").alias(\"message_key\"),\n",
    "            col(\"topic\"),\n",
    "            col(\"partition\"),\n",
    "            col(\"offset\"),\n",
    "            col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
    "            length(col(\"value\")).alias(\"message_size_bytes\"),\n",
    "            col(\"value\")  # Raw Avro binary data\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Streaming DataFrame created for {topic_name}\")\n",
    "        print(f\"   📊 Schema (what each streaming record looks like):\")\n",
    "        structured_stream.printSchema()\n",
    "        \n",
    "        return structured_stream\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating streaming DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"🛒 CREATING ORDERS STREAM:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "orders_stream = create_streaming_dataframe(\"orders.v1\")\n",
    "\n",
    "if orders_stream:\n",
    "    print(f\"\\n🎓 KEY CONCEPTS:\")\n",
    "    print(f\"   • This DataFrame represents INFINITE data - it never ends\")\n",
    "    print(f\"   • isStreaming = {orders_stream.isStreaming}\")\n",
    "    print(f\"   • You can't call .show() or .collect() on it directly\")\n",
    "    print(f\"   • You need a 'streaming query' to process the data\")\n",
    "    print(f\"   • Spark processes data in micro-batches (e.g., every 2 seconds)\")\n",
    "else:\n",
    "    print(\"❌ Failed to create orders stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51d9e5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💾 Lesson 5: Checkpoints - The Fault Tolerance Foundation\n",
    "\n",
    "**Goal:** Understand why checkpoints are critical for production streaming.\n",
    "\n",
    "Streaming applications run 24/7. Networks fail, machines crash, code gets deployed. Without checkpoints, you'd lose progress and either miss data or reprocess everything. Checkpoints are your streaming safety net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458975f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(\"💾 CHECKPOINTING DEMONSTRATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if orders_stream:\n",
    "    print(\"🔍 What checkpoints store:\")\n",
    "    print(\"   • Stream metadata (offsets, batch IDs)\")\n",
    "    print(\"   • Query progress information\")\n",
    "    print(\"   • State store data for aggregations\")\n",
    "    print(\"   • Watermark information for event time\")\n",
    "    \n",
    "    # Create a streaming query with memory sink (perfect for Jupyter)\n",
    "    checkpoint_demo_query = orders_stream.select(\n",
    "        col(\"message_key\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"message_size_bytes\"),\n",
    "        # Extract hex preview for educational purposes\n",
    "        expr(\"hex(substring(value, 1, 20))\").alias(\"avro_hex_preview\")\n",
    "    ).writeStream \\\n",
    "        .queryName(\"checkpoint_demo\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/streaming-lesson-checkpoint/demo\") \\\n",
    "        .trigger(processingTime=\"3 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    print(f\"\\n✅ Streaming query started with checkpointing!\")\n",
    "    print(f\"   📍 Query name: {checkpoint_demo_query.name}\")\n",
    "    print(f\"   📁 Checkpoint location: /tmp/streaming-lesson-checkpoint/demo\")\n",
    "    print(f\"   ⏱️ Processing trigger: every 3 seconds\")\n",
    "    print(f\"\\n🔄 Watching streaming progress (15 seconds)...\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            current_data = spark.sql(\"SELECT COUNT(*) as total FROM checkpoint_demo\")\n",
    "            total_messages = current_data.collect()[0].total\n",
    "            progress = checkpoint_demo_query.lastProgress\n",
    "            \n",
    "            print(f\"   📊 Progress check {i+1}/5:\")\n",
    "            print(f\"      Messages processed: {total_messages}\")\n",
    "            \n",
    "            if progress:\n",
    "                batch_id = progress.get('batchId', 'N/A')\n",
    "                input_rate = progress.get('inputRowsPerSecond', 0)\n",
    "                print(f\"      Current batch: {batch_id}\")\n",
    "                print(f\"      Input rate: {input_rate:.1f} rows/sec\")\n",
    "            if total_messages > 0:\n",
    "                sample = spark.sql(\"SELECT message_key, offset, message_size_bytes FROM checkpoint_demo ORDER BY offset DESC LIMIT 2\")\n",
    "                rows = sample.collect()\n",
    "                print(f\"      Latest messages:\")\n",
    "                for row in rows:\n",
    "                    print(f\"        Key: {row.message_key}, Offset: {row.offset}, Size: {row.message_size_bytes}B\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ⚠️ Progress check {i+1}: {e}\")\n",
    "    checkpoint_demo_query.stop()\n",
    "    print(f\"\\n🛑 Streaming query stopped\")\n",
    "    \n",
    "    print(f\"\\n🎓 CHECKPOINT BENEFITS:\")\n",
    "    print(f\"   ✅ Exactly-once processing guarantees\")\n",
    "    print(f\"   ✅ Fault tolerance - resume from failure point\")\n",
    "    print(f\"   ✅ State preservation for aggregations\")\n",
    "    print(f\"   ✅ No data loss or duplication\")\n",
    "    \n",
    "    print(f\"\\n⚠️ CHECKPOINT CONSIDERATIONS:\")\n",
    "    print(f\"   • Choose reliable storage (HDFS, S3, not local /tmp in production)\")\n",
    "    print(f\"   • Checkpoint format is tied to Spark version\")\n",
    "    print(f\"   • Schema changes may require checkpoint reset\")\n",
    "    print(f\"   • Checkpoint size grows with state (aggregations)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No stream available for checkpoint demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad52cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🌊 Lesson 6: Advanced Streaming - Avro Decoding in Real-Time\n",
    "\n",
    "**Goal:** Combine everything - stream processing with Avro decoding.\n",
    "\n",
    "Now let's put it all together: process infinite Kafka streams, decode Avro messages in real-time, and display business-readable data. This is production-level streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dddea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 ADVANCED STREAMING WITH AVRO DECODING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if orders_stream:\n",
    "    print(\"🎯 This demonstrates production-level streaming:\")\n",
    "    print(\"   • Infinite data processing\")\n",
    "    print(\"   • Real-time Avro decoding\")\n",
    "    print(\"   • Business data extraction\")\n",
    "    print(\"   • Fault-tolerant checkpointing\")\n",
    "    advanced_query = orders_stream.select(\n",
    "        col(\"message_key\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"kafka_timestamp\"),\n",
    "        col(\"value\"),  # Full Avro binary for decoding\n",
    "        col(\"message_size_bytes\")\n",
    "    ).writeStream \\\n",
    "        .queryName(\"advanced_avro_streaming\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"memory\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/streaming-lesson-checkpoint/advanced\") \\\n",
    "        .trigger(processingTime=\"2 seconds\") \\\n",
    "        .start()\n",
    "\n",
    "    print(f\"\\n✅ Advanced streaming query started!\")\n",
    "    print(f\"   🔧 Processing: Kafka → Spark → Avro decode → Business data\")\n",
    "    print(f\"   📊 Collecting and decoding live data...\")\n",
    "    for i in range(6):\n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            current_data = spark.sql(\"SELECT * FROM advanced_avro_streaming ORDER BY offset DESC LIMIT 1\")\n",
    "            data_count = current_data.count()\n",
    "            \n",
    "            if data_count > 0:\n",
    "                print(f\"\\n📊 LIVE UPDATE {i+1}/6:\")\n",
    "                print(\"─\" * 40)\n",
    "                \n",
    "                row = current_data.collect()[0]\n",
    "                print(f\"📍 Kafka metadata:\")\n",
    "                print(f\"   Key: {row.message_key}\")\n",
    "                print(f\"   Offset: {row.offset}\")\n",
    "                print(f\"   Timestamp: {row.kafka_timestamp}\")\n",
    "                print(f\"   Size: {row.message_size_bytes} bytes\")\n",
    "                try:\n",
    "                    message_bytes = row.value\n",
    "                    if message_bytes and len(message_bytes) >= 5:\n",
    "                        magic_byte = message_bytes[0]\n",
    "                        schema_id = int.from_bytes(message_bytes[1:5], byteorder='big')\n",
    "                        avro_payload = message_bytes[5:]\n",
    "                        \n",
    "                        if magic_byte == 0:\n",
    "                            try:\n",
    "                                import io\n",
    "                                import avro.schema\n",
    "                                import avro.io\n",
    "                                \n",
    "                                schema = sr_client.get_schema(schema_id)\n",
    "                                avro_schema = avro.schema.parse(schema.schema_str)\n",
    "                                \n",
    "                                bytes_reader = io.BytesIO(avro_payload)\n",
    "                                decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "                                reader = avro.io.DatumReader(avro_schema)\n",
    "                                \n",
    "                                decoded = reader.read(decoder)\n",
    "                                \n",
    "                                print(f\"\\n🎯 DECODED BUSINESS DATA:\")\n",
    "                                for key, value in decoded.items():\n",
    "                                    if isinstance(value, (int, float)):\n",
    "                                        print(f\"   {key}: {value}\")\n",
    "                                    elif isinstance(value, str):\n",
    "                                        safe_value = ''.join(c if ord(c) < 128 else '?' for c in value)\n",
    "                                        print(f\"   {key}: '{safe_value}'\")\n",
    "                                    else:\n",
    "                                        print(f\"   {key}: {str(value)}\")\n",
    "\n",
    "                                if 'ts' in decoded and isinstance(decoded['ts'], (int, float)):\n",
    "                                    from datetime import datetime\n",
    "                                    ts_readable = datetime.fromtimestamp(decoded['ts'] / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                    print(f\"   event_time: {ts_readable}\")\n",
    "                                \n",
    "                                print(f\"   ✅ Real-time Avro decoding successful!\")\n",
    "                                    \n",
    "                            except Exception as decode_error:\n",
    "                                print(f\"   ⚠️ Avro decode error: {decode_error}\")\n",
    "                        else:\n",
    "                            print(f\"   ⚠️ Invalid magic byte: {magic_byte}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Processing error: {e}\")\n",
    "            else:\n",
    "                print(f\"⏳ Update {i+1}/6: Waiting for streaming data...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Update {i+1}/6: {e}\")\n",
    "    advanced_query.stop()\n",
    "    print(f\"\\n🛑 Advanced streaming query stopped\")\n",
    "    \n",
    "    print(f\"\\n🎓 WHAT YOU JUST SAW:\")\n",
    "    print(f\"   🌊 Infinite data stream processing\")\n",
    "    print(f\"   📋 Schema Registry integration\")\n",
    "    print(f\"   🔧 Real-time Avro decoding\")\n",
    "    print(f\"   💾 Fault-tolerant checkpointing\")\n",
    "    print(f\"   📊 Business data extraction from binary streams\")\n",
    "    print(f\"   ⚡ This is how production streaming systems work!\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No stream available for advanced demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d3be8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 Lesson Summary & Production Insights\n",
    "\n",
    "**Congratulations!** You've learned the fundamentals of modern streaming architecture.\n",
    "\n",
    "### 🎓 What You Learned\n",
    "\n",
    "1. **Prerequisites** → Data Forge's generator creates realistic streaming data\n",
    "2. **Data Examination** → Streaming data is binary Avro, not JSON\n",
    "3. **Avro Superiority** → 30-50% smaller, faster, type-safe, evolvable\n",
    "4. **Schema Registry** → Enables safe schema evolution without breaking consumers\n",
    "5. **Spark Streaming** → Treats infinite streams as \"growing tables\"\n",
    "6. **Checkpoints** → Critical for exactly-once processing and fault tolerance\n",
    "\n",
    "### 🏭 Production Patterns\n",
    "\n",
    "**You're now ready for real-world streaming:**\n",
    "\n",
    "- **Schema Evolution** → Add/remove fields without downtime\n",
    "- **Fault Tolerance** → Streams survive failures and restarts\n",
    "- **Exactly-Once Processing** → No data loss or duplication\n",
    "- **Type Safety** → Avro prevents runtime errors\n",
    "- **Performance** → Binary encoding reduces costs\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "**Explore more Data Forge capabilities:**\n",
    "\n",
    "```bash\n",
    "# Explore with Trino SQL\n",
    "# Visit http://localhost:8081 for Trino UI\n",
    "\n",
    "# Build dashboards in Superset\n",
    "# Visit http://localhost:8088 (admin/admin)\n",
    "```\n",
    "\n",
    "**Additional learning resources:**\n",
    "- `notebooks/lessons/streaming` → More streaming examples\n",
    "- `docs/` → Service-specific guides\n",
    "- Data Forge README → Architecture overview\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a40d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧹 LESSON CLEANUP:\")\n",
    "print(\"=\" * 30)\n",
    "active_queries = spark.streams.active\n",
    "if active_queries:\n",
    "    print(f\"🛑 Stopping {len(active_queries)} active streaming queries...\")\n",
    "    for query in active_queries:\n",
    "        query.stop()\n",
    "        print(f\"   ✅ Stopped: {query.name}\")\n",
    "else:\n",
    "    print(\"ℹ️ No active queries to stop\")\n",
    "try:\n",
    "    import shutil\n",
    "    if os.path.exists(\"/tmp/streaming-lesson-checkpoint\"):\n",
    "        shutil.rmtree(\"/tmp/streaming-lesson-checkpoint\")\n",
    "        print(\"🗑️ Cleaned up checkpoint directory\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Checkpoint cleanup: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Lesson cleanup complete!\")\n",
    "print(f\"💡 Spark session remains active for further experimentation\")\n",
    "print(f\"🎉 You've mastered streaming fundamentals with Data Forge!\")\n",
    "\n",
    "# Optional: Uncomment to stop Spark completely\n",
    "# spark.stop()\n",
    "# print(\"🏁 Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc7e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
