{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f37b49",
   "metadata": {},
   "source": [
    "# üßä Spark + Iceberg DML\n",
    "\n",
    "Iceberg is an open table format for large analytic datasets. It brings ACID transactions, schema evolution, and time travel to data lakes.\n",
    "\n",
    "## Why Iceberg?\n",
    "Iceberg enables reliable, scalable, and flexible data management on object storage. It is ideal for the bronze (raw) layer, where data is ingested as-is but must remain queryable, auditable, and mutable.\n",
    "\n",
    "## üß© Roles in the Stack\n",
    "- **MinIO**: S3-compatible object storage for all table data and metadata files.\n",
    "- **Hive Metastore**: Stores table and database metadata, enabling Spark and Trino to discover and manage Iceberg tables.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Spark session configuration for Iceberg\n",
    "- S3/MinIO integration\n",
    "- Iceberg table creation and DML (CREATE, INSERT, UPDATE)\n",
    "- Time travel and metadata queries\n",
    "- Validation and federated access via Trino\n",
    "\n",
    "üõë **Prerequisites:** Core services must be running (`docker compose --profile core up -d`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190db0f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Environment Setup\n",
    "\n",
    "Configure connection URLs and credentials from Docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ROOT_USER', 'minio')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_ROOT_PASSWORD', 'minio123')\n",
    "HIVE_METASTORE_URI = \"thrift://hive-metastore:9083\"\n",
    "TRINO_URL = \"http://trino:8080\"\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "\n",
    "S3_ENDPOINT = \"minio:9000\"\n",
    "S3_ACCESS_KEY = MINIO_ACCESS_KEY\n",
    "S3_SECRET_KEY = MINIO_SECRET_KEY\n",
    "\n",
    "print(\"üîß Environment configured\")\n",
    "print(f\"MinIO: {MINIO_ENDPOINT}\")\n",
    "print(f\"Hive Metastore: {HIVE_METASTORE_URI}\")\n",
    "print(f\"Trino: {TRINO_URL}\")\n",
    "print(f\"Spark Master: {SPARK_MASTER}\")\n",
    "\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15939d9",
   "metadata": {},
   "source": [
    "## üßä Spark Session with Iceberg\n",
    "\n",
    "Configure Spark with latest Iceberg JARs and proper catalog settings.\n",
    "\n",
    "The configuration includes:\n",
    "- **Iceberg runtime JARs** via `spark.jars.packages`\n",
    "- **Hive catalog integration** for metadata management  \n",
    "- **S3/MinIO connectivity** for data storage\n",
    "- **SQL extensions** for Iceberg operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f41189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "print(\"üöÄ Creating Spark session with Iceberg REST Catalog support...\")\n",
    "\n",
    "packages = [\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"software.amazon.awssdk:bundle:2.20.158\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"IcebergDMLDemo\") \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.type\", \"rest\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.uri\", \"http://hive-metastore:9001/iceberg\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3a://iceberg/warehouse\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.endpoint\", f\"http://{S3_ENDPOINT}\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", S3_ACCESS_KEY) \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", S3_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"us-east-1\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.region\", \"us-east-1\") \\\n",
    "        .config(\"spark.executorEnv.AWS_REGION\", \"us-east-1\") \\\n",
    "        .config(\"spark.executorEnv.AWS_DEFAULT_REGION\", \"us-east-1\") \\\n",
    "        .config(\"spark.executorEnv.aws.region\", \"us-east-1\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Daws.region=us-east-1\") \\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Daws.region=us-east-1\") \\\n",
    "        .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\") \\\n",
    "        .config(\"spark.sql.defaultCatalog\", \"iceberg\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.uris\", HIVE_METASTORE_URI) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"‚úÖ Spark session created successfully with Iceberg REST Catalog\")\n",
    "    print(f\"üìä Spark UI available at: http://localhost:8088\")\n",
    "    print(f\"üîß Spark version: {spark.version}\")\n",
    "    print(f\"üåê Iceberg REST Catalog: http://hive-metastore:9001/iceberg\")\n",
    "    \n",
    "except Exception as e:\n",
    "        print(f\"‚ùå All attempts failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40991b47",
   "metadata": {},
   "source": [
    "## ü™£ MinIO Bucket Setup\n",
    "\n",
    "Ensure the iceberg bucket exists in MinIO for Iceberg table storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "def ensure_bucket_exists(bucket_name):\n",
    "    \"\"\"Create bucket if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"‚úÖ Bucket '{bucket_name}' already exists\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        error_code = int(e.response['Error']['Code'])\n",
    "        if error_code == 404:\n",
    "            try:\n",
    "                s3_client.create_bucket(Bucket=bucket_name)\n",
    "                print(f\"‚úÖ Created bucket '{bucket_name}'\")\n",
    "                return True\n",
    "            except Exception as create_error:\n",
    "                print(f\"‚ùå Failed to create bucket '{bucket_name}': {create_error}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"‚ùå Error checking bucket '{bucket_name}': {e}\")\n",
    "            return False\n",
    "\n",
    "bucket_ready = ensure_bucket_exists(\"iceberg\")\n",
    "buckets = s3_client.list_buckets()\n",
    "print(f\"\\nüìÇ Available buckets: {[b['Name'] for b in buckets['Buckets']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9fda5",
   "metadata": {},
   "source": [
    "## üîç Verify Iceberg Integration\n",
    "\n",
    "Test that Spark can access Iceberg catalogs and the metastore connection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ade88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Testing Iceberg catalog integration...\")\n",
    "\n",
    "try:\n",
    "    catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "    print(\"üìã Available Catalogs:\")\n",
    "    catalogs_df.show()\n",
    "    print(\"‚úÖ Testing iceberg catalog access...\")\n",
    "    databases_df = spark.sql(\"SHOW DATABASES IN iceberg\")\n",
    "    print(\"\\nüìã Databases in iceberg catalog:\")\n",
    "    databases_df.show()\n",
    "    \n",
    "    print(\"‚úÖ Iceberg catalog integration verified\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Catalog test failed: {str(e)}\")\n",
    "    print(\"üîß This might be expected if no databases exist yet in iceberg catalog\")\n",
    "    try:\n",
    "        print(\"üîÑ Testing catalog write access...\")\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg.test_connectivity\")\n",
    "        spark.sql(\"DROP DATABASE IF EXISTS iceberg.test_connectivity\")\n",
    "        print(\"‚úÖ Catalog write access confirmed\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ö†Ô∏è  Catalog write test also failed: {str(e2)}\")\n",
    "        print(\"   Check if Hive Metastore and MinIO are properly connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c49ec",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Create Database and Tables\n",
    "\n",
    "Create a sample database and demonstrate Iceberg table creation with different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"temp_iceberg_dml\"\n",
    "\n",
    "print(f\"üèóÔ∏è Creating database: {database_name}\")\n",
    "\n",
    "try:\n",
    "    print(\"üîç Testing catalog access...\")\n",
    "    spark.sql(\"SHOW DATABASES IN iceberg\").show()\n",
    "    database_location = f\"s3a://iceberg/warehouse/{database_name}.db\"\n",
    "    print(f\"üìç Database location: {database_location}\")\n",
    "    \n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS iceberg.{database_name} LOCATION '{database_location}'\")\n",
    "    print(f\"‚úÖ Database '{database_name}' created successfully\")\n",
    "    spark.sql(f\"USE iceberg.{database_name}\")\n",
    "    current_db = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "    print(f\"üìç Current database: {current_db}\")\n",
    "    print(\"\\nüìã Available databases:\")\n",
    "    spark.sql(\"SHOW DATABASES IN iceberg\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database creation failed: {str(e)}\")\n",
    "    print(\"\\nüîß Troubleshooting suggestions:\")\n",
    "    print(\"   1. Check if Hive Metastore is running and accessible\")\n",
    "    print(\"   2. Verify MinIO bucket 'iceberg' exists and is accessible\")\n",
    "    print(\"   3. Check S3A filesystem configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d4c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßä Creating Iceberg table: customers\")\n",
    "customers_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customers (\n",
    "    customer_id BIGINT,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    email STRING,\n",
    "    registration_date DATE,\n",
    "    last_login TIMESTAMP,\n",
    "    lifetime_value DOUBLE,\n",
    "    segment STRING\n",
    ") USING iceberg\n",
    "PARTITIONED BY (segment)\n",
    "TBLPROPERTIES (\n",
    "    'write.target-file-size-bytes'='134217728',\n",
    "    'write.parquet.compression-codec'='snappy'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(customers_ddl)\n",
    "    print(\"‚úÖ Customers table created successfully with Iceberg format\")\n",
    "    spark.sql(\"DESCRIBE EXTENDED customers\").show(20, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create Iceberg table: {str(e)}\")\n",
    "    print(\"\\nüîß This error suggests S3A/MinIO connectivity issues\")\n",
    "    print(f\"‚ùå All approaches failed: {str(e3)}\")\n",
    "    print(\"\\nüí° Next steps:\")\n",
    "    print(\"   1. Ensure docker compose --profile core is running\")\n",
    "    print(\"   2. Check MinIO console at http://localhost:9001\")\n",
    "    print(\"   3. Verify Hive Metastore logs for connection issues\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßä Creating Iceberg table: orders\")\n",
    "\n",
    "orders_ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS orders (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    order_date DATE,\n",
    "    order_timestamp TIMESTAMP,\n",
    "    total_amount DOUBLE,\n",
    "    status STRING,\n",
    "    payment_method STRING,\n",
    "    shipping_address STRING\n",
    ") USING iceberg\n",
    "PARTITIONED BY (months(order_date))\n",
    "TBLPROPERTIES (\n",
    "    'write.target-file-size-bytes'='268435456',\n",
    "    'write.parquet.compression-codec'='zstd'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(orders_ddl)\n",
    "    print(\"‚úÖ Orders table created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create orders table: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a380b88",
   "metadata": {},
   "source": [
    "## üìù Insert Sample Data\n",
    "\n",
    "Insert realistic sample data into our Iceberg tables using Spark SQL DML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f933a44",
   "metadata": {},
   "source": [
    "### üìù DML Operations with Iceberg\n",
    "\n",
    "Iceberg supports full ACID DML: CREATE, INSERT, UPDATE, DELETE. Each operation is atomic and tracked in table metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8582ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "import random\n",
    "\n",
    "print(\"üìù Inserting sample customer data...\")\n",
    "customer_data = []\n",
    "segments = ['Premium', 'Standard', 'Basic']\n",
    "domains = ['gmail.com', 'yahoo.com', 'company.com', 'outlook.com']\n",
    "\n",
    "for i in range(1, 101):\n",
    "    customer_data.append((\n",
    "        i,\n",
    "        f\"Customer_{i:03d}\",\n",
    "        f\"Last_{i:03d}\",\n",
    "        f\"customer_{i:03d}@{random.choice(domains)}\",\n",
    "        date(2022, 1, 1) + timedelta(days=random.randint(0, 1000)),\n",
    "        datetime.now() - timedelta(hours=random.randint(1, 8760)),\n",
    "        round(random.uniform(100.0, 10000.0), 2),\n",
    "        random.choice(segments)\n",
    "    ))\n",
    "\n",
    "customers_df = spark.createDataFrame(\n",
    "    customer_data,\n",
    "    [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"registration_date\", \"last_login\", \"lifetime_value\", \"segment\"]\n",
    ")\n",
    "\n",
    "customers_df.writeTo(\"customers\").append()\n",
    "print(\"‚úÖ Customer data inserted successfully\")\n",
    "\n",
    "customer_count = spark.sql(\"SELECT COUNT(*) as count FROM customers\").collect()[0]['count']\n",
    "print(f\"üìä Total customers: {customer_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Inserting sample order data...\")\n",
    "\n",
    "order_data = []\n",
    "statuses = ['Pending', 'Processing', 'Shipped', 'Delivered', 'Cancelled']\n",
    "payment_methods = ['Credit Card', 'PayPal', 'Bank Transfer', 'Cash']\n",
    "\n",
    "for i in range(1, 301):\n",
    "    order_date = date(2024, 1, 1) + timedelta(days=random.randint(0, 250))\n",
    "    order_data.append((\n",
    "        i,\n",
    "        random.randint(1, 100),\n",
    "        order_date,\n",
    "        datetime.combine(order_date, datetime.min.time()) + timedelta(hours=random.randint(0, 23), minutes=random.randint(0, 59)),\n",
    "        round(random.uniform(25.0, 2500.0), 2),\n",
    "        random.choice(statuses),\n",
    "        random.choice(payment_methods),\n",
    "        f\"Address {i}, City, State {random.randint(10000, 99999)}\"\n",
    "    ))\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    order_data,\n",
    "    [\"order_id\", \"customer_id\", \"order_date\", \"order_timestamp\", \"total_amount\", \"status\", \"payment_method\", \"shipping_address\"]\n",
    ")\n",
    "\n",
    "orders_df.writeTo(\"orders\").append()\n",
    "print(\"‚úÖ Order data inserted successfully\")\n",
    "\n",
    "order_count = spark.sql(\"SELECT COUNT(*) as count FROM orders\").collect()[0]['count']\n",
    "print(f\"üìä Total orders: {order_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be5a9fb",
   "metadata": {},
   "source": [
    "## üîç Query and Analyze Data\n",
    "\n",
    "Demonstrate various Iceberg features through SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Running analytics queries on Iceberg tables...\")\n",
    "print(\"\\nüéØ Customer Segment Analysis:\")\n",
    "segment_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        AVG(lifetime_value) as avg_lifetime_value,\n",
    "        MAX(lifetime_value) as max_lifetime_value\n",
    "    FROM customers \n",
    "    GROUP BY segment \n",
    "    ORDER BY avg_lifetime_value DESC\n",
    "\"\"\")\n",
    "segment_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ab9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÖ Monthly Order Trends:\")\n",
    "monthly_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(order_date) as year,\n",
    "        MONTH(order_date) as month,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_order_value\n",
    "    FROM orders \n",
    "    GROUP BY YEAR(order_date), MONTH(order_date)\n",
    "    ORDER BY year, month\n",
    "\"\"\")\n",
    "monthly_trends.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Customer-Order Join Analysis:\")\n",
    "customer_order_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.segment,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        SUM(o.total_amount) as total_spent,\n",
    "        AVG(o.total_amount) as avg_order_value\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.segment, c.first_name, c.last_name\n",
    "    HAVING total_orders > 0\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "customer_order_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640c1c2",
   "metadata": {},
   "source": [
    "## üï∞Ô∏è Iceberg Time Travel\n",
    "\n",
    "Demonstrate Iceberg's time travel capabilities by viewing table snapshots and history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf3725",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üï∞Ô∏è Iceberg Time Travel Features\")\n",
    "print(\"\\nüì∏ Table Snapshots for customers:\")\n",
    "try:\n",
    "    snapshots = spark.sql(\"SELECT * FROM iceberg.temp_iceberg_dml.customers.snapshots\")\n",
    "    snapshots.select(\"committed_at\", \"snapshot_id\", \"operation\", \"summary\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"Snapshots query failed: {e}\")\n",
    "\n",
    "print(\"\\nüìú Table History for customers:\")\n",
    "try:\n",
    "    history = spark.sql(\"SELECT * FROM iceberg.temp_iceberg_dml.customers.history\")\n",
    "    history.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"History query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9294a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Performing UPDATE operation for time travel demo...\")\n",
    "update_sql = \"\"\"\n",
    "    UPDATE customers \n",
    "    SET lifetime_value = lifetime_value * 1.1,\n",
    "        segment = CASE \n",
    "            WHEN lifetime_value > 5000 THEN 'Premium'\n",
    "            WHEN lifetime_value > 1000 THEN 'Standard'\n",
    "            ELSE 'Basic'\n",
    "        END\n",
    "    WHERE segment = 'Premium'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(update_sql)\n",
    "    print(\"‚úÖ Update operation completed\")\n",
    "    updated_counts = spark.sql(\"\"\"\n",
    "        SELECT segment, COUNT(*) as count \n",
    "        FROM customers \n",
    "        GROUP BY segment \n",
    "        ORDER BY segment\n",
    "    \"\"\")\n",
    "    print(\"\\nüìä Updated segment distribution:\")\n",
    "    updated_counts.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Update failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb61e0c",
   "metadata": {},
   "source": [
    "## üîó Trino Integration Verification\n",
    "\n",
    "Verify that our Iceberg tables are accessible through Trino for federated queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a555fa",
   "metadata": {},
   "source": [
    "## üîó Why Validate and Access via Trino?\n",
    "\n",
    "Trino is a distributed SQL query engine that can access Iceberg tables directly via the Hive Metastore. This allows us to:\n",
    "- Validate that tables are discoverable and queryable outside Spark.\n",
    "- Enable federated analytics across multiple data sources.\n",
    "- Provide a single SQL interface for BI tools and analysts.\n",
    "\n",
    "Using Trino ensures our Iceberg tables are interoperable and production-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Testing Trino integration with Iceberg tables...\")\n",
    "\n",
    "try:\n",
    "    from trino.dbapi import connect as trino_connect\n",
    "    import pandas as pd\n",
    "    \n",
    "    trino_conn = trino_connect(\n",
    "        host='trino',\n",
    "        port=8080,\n",
    "        user='admin',\n",
    "        catalog='iceberg',\n",
    "        schema='temp_iceberg_dml'\n",
    "    )\n",
    "    \n",
    "    def query_trino(sql):\n",
    "        cursor = trino_conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        data = cursor.fetchall()\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    print(\"üìã Available schemas in Trino iceberg catalog:\")\n",
    "    schemas = query_trino(\"SHOW SCHEMAS\")\n",
    "    print(schemas)\n",
    "    if 'temp_iceberg_dml' in schemas['Schema'].values:\n",
    "        print(\"\\nüìã Tables in temp_iceberg_dml schema via Trino:\")\n",
    "        tables = query_trino(\"SHOW TABLES\")\n",
    "        print(tables)\n",
    "        print(\"\\nüìä Sample data via Trino:\")\n",
    "        sample_data = query_trino(\"SELECT segment, COUNT(*) as count FROM customers GROUP BY segment\")\n",
    "        print(sample_data)\n",
    "        \n",
    "        print(\"‚úÖ Trino integration verified successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è temp_iceberg_dml schema not yet visible in Trino\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"üì¶ Trino client not available, skipping integration test\")\n",
    "except Exception as e:\n",
    "    print(f\"üîó Trino integration test failed: {str(e)}\")\n",
    "    print(\"   This might be expected if Trino catalog refresh is needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbbf12",
   "metadata": {},
   "source": [
    "## üßπ Table Maintenance\n",
    "\n",
    "Demonstrate Iceberg table maintenance operations like OPTIMIZE and EXPIRE SNAPSHOTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84481121",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßπ Performing table maintenance operations...\")\n",
    "print(\"üìÇ Table files before optimization:\")\n",
    "try:\n",
    "    files_before = spark.sql(\"SELECT * FROM iceberg.temp_iceberg_dml.customers.files\")\n",
    "    file_count_before = files_before.count()\n",
    "    print(f\"   Files count: {file_count_before}\")\n",
    "    if file_count_before > 0:\n",
    "        files_before.select(\"file_size_in_bytes\").describe().show()\n",
    "except Exception as e:\n",
    "    print(f\"   Files query failed: {e}\")\n",
    "print(\"\\n‚ö° Optimizing customers table...\")\n",
    "try:\n",
    "    optimize_result = spark.sql(\"CALL iceberg.system.rewrite_data_files('temp_iceberg_dml.customers')\")\n",
    "    optimize_result.show()\n",
    "    print(\"‚úÖ Table optimization completed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Optimization failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üóëÔ∏è Cleaning up old snapshots...\")\n",
    "\n",
    "try:\n",
    "    expire_result = spark.sql(f\"\"\"\n",
    "        CALL iceberg.system.expire_snapshots(\n",
    "            table => 'temp_iceberg_dml.customers',\n",
    "            older_than => TIMESTAMP '{datetime.now() - timedelta(hours=1)}'\n",
    "        )\n",
    "    \"\"\")\n",
    "    expire_result.show()\n",
    "    print(\"‚úÖ Snapshot cleanup completed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Snapshot cleanup failed: {str(e)}\")\n",
    "    print(\"   This might be expected if no old snapshots exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5844c",
   "metadata": {},
   "source": [
    "## üìä Performance and Metadata Analysis\n",
    "\n",
    "Analyze table metadata and performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Table Metadata Analysis\")\n",
    "tables_to_analyze = ['customers', 'orders']\n",
    "\n",
    "for table_name in tables_to_analyze:\n",
    "    print(f\"\\nüîç Analyzing table: {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        describe_result = spark.sql(f\"DESCRIBE EXTENDED iceberg.temp_iceberg_dml.{table_name}\")\n",
    "        print(f\"   Schema and properties:\")\n",
    "        describe_result.show(20, truncate=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Analysis failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d13f4",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "- Iceberg brings reliability and flexibility to data lakes.\n",
    "- MinIO stores the data; Hive Metastore tracks the metadata.\n",
    "- Spark and Trino both access Iceberg tables for analytics and validation.\n",
    "- All basic DML operations are supported with ACID guarantees.\n",
    "\n",
    "For more, see the project README and docs/guidelines.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d89df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Spark + Iceberg Integration Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    customer_count = spark.sql(\"SELECT COUNT(*) FROM iceberg.temp_iceberg_dml.customers\").collect()[0][0]\n",
    "    order_count = spark.sql(\"SELECT COUNT(*) FROM iceberg.temp_iceberg_dml.orders\").collect()[0][0]\n",
    "    \n",
    "    print(f\"‚úÖ Successfully created Iceberg tables:\")\n",
    "    print(f\"   üìä Customers: {customer_count:,} records\")\n",
    "    print(f\"   üìä Orders: {order_count:,} records\")\n",
    "    print(f\"\\nüìÇ Database structure:\")\n",
    "    tables = spark.sql(\"SHOW TABLES IN iceberg.temp_iceberg_dml\")\n",
    "    for row in tables.collect():\n",
    "        print(f\"   üßä {row['tableName']} (iceberg)\")\n",
    "    \n",
    "    print(f\"\\nüîß Key configurations demonstrated:\")\n",
    "    print(f\"   üì¶ JAR management via spark.jars.packages\")\n",
    "    print(f\"   üóÇÔ∏è Hive Metastore integration\")\n",
    "    print(f\"   ü™£ MinIO/S3 storage backend\")\n",
    "    print(f\"   üï∞Ô∏è Time travel capabilities\")\n",
    "    print(f\"   üßπ Table maintenance operations\")\n",
    "    print(f\"   üîó Trino federated query access\")\n",
    "    \n",
    "    print(f\"\\nüöÄ Next steps to explore:\")\n",
    "    print(f\"   üìà Schema evolution with ALTER TABLE\")\n",
    "    print(f\"   üîÑ Streaming ingestion with Structured Streaming\")\n",
    "    print(f\"   üìä Advanced analytics with window functions\")\n",
    "    print(f\"   üèóÔ∏è Data iceberg patterns with Delta/Iceberg\")\n",
    "    print(f\"   üîç Query optimization and performance tuning\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Summary generation failed: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüéâ Iceberg integration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038115d",
   "metadata": {},
   "source": [
    "## üßπ Cleanup: Drop Tables and Database\n",
    "\n",
    "The following cell will remove all demo tables and the database created during this notebook. Use with caution if you want to keep your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"üßπ Dropping tables and database...\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS customers\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS orders\")\n",
    "    spark.sql(\"DROP DATABASE IF EXISTS temp_iceberg_dml\")\n",
    "    print(\"‚úÖ Cleanup complete: tables and database dropped.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cleanup failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
