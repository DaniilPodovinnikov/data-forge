{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca96646",
   "metadata": {},
   "source": [
    "# Data Forge Service Connections Validation\n",
    "\n",
    "This notebook validates connections to all services in the Data Forge platform.\n",
    "\n",
    "## Services Covered:\n",
    "- üìä **PostgreSQL** - Primary database\n",
    "- üöÄ **ClickHouse** - Analytics database \n",
    "- üóÑÔ∏è **MinIO** - Object storage\n",
    "- üì® **Kafka** - Message streaming\n",
    "- ‚ö° **Trino** - SQL query engine\n",
    "- üî• **Spark** - Distributed computing\n",
    "- üìà **Superset** - Business intelligence\n",
    "- üå™Ô∏è **Airflow** - Workflow orchestration\n",
    "- ‚ö° **Redis** - Caching layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f752ea",
   "metadata": {},
   "source": [
    "## Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0f9545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:25:57.636045Z",
     "iopub.status.busy": "2025-09-04T09:25:57.635684Z",
     "iopub.status.idle": "2025-09-04T09:25:58.665912Z",
     "shell.execute_reply": "2025-09-04T09:25:58.665329Z",
     "shell.execute_reply.started": "2025-09-04T09:25:57.636024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üìÖ Validation started at: 2025-09-04 09:25:58.663656\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import connection libraries\n",
    "import psycopg2\n",
    "import clickhouse_connect\n",
    "import boto3\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import redis\n",
    "import requests\n",
    "from trino.dbapi import connect as trino_connect\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìÖ Validation started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88bf71b",
   "metadata": {},
   "source": [
    "## Connection Configuration\n",
    "\n",
    "Reading connection parameters from environment variables passed by Docker Compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d66e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:04.565983Z",
     "iopub.status.busy": "2025-09-04T09:26:04.565476Z",
     "iopub.status.idle": "2025-09-04T09:26:04.571530Z",
     "shell.execute_reply": "2025-09-04T09:26:04.570998Z",
     "shell.execute_reply.started": "2025-09-04T09:26:04.565962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded from environment variables\n",
      "üóÑÔ∏è PostgreSQL: postgres:5432\n",
      "üìä ClickHouse: clickhouse:8123\n",
      "‚òÅÔ∏è MinIO: http://minio:9000\n",
      "üì® Kafka: ['kafka:9092']\n"
     ]
    }
   ],
   "source": [
    "# Database configurations\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'postgres',\n",
    "    'port': 5432,\n",
    "    'database': os.getenv('POSTGRES_DB', 'metastore'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'admin'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'admin')\n",
    "}\n",
    "\n",
    "CLICKHOUSE_CONFIG = {\n",
    "    'host': 'clickhouse',\n",
    "    'port': 8123,\n",
    "    'database': os.getenv('CLICKHOUSE_DB', 'analytics'),\n",
    "    'username': os.getenv('CLICKHOUSE_USER', 'admin'),\n",
    "    'password': os.getenv('CLICKHOUSE_PASSWORD', 'admin')\n",
    "}\n",
    "\n",
    "# Object Storage configuration\n",
    "MINIO_CONFIG = {\n",
    "    'endpoint_url': 'http://minio:9000',\n",
    "    'aws_access_key_id': os.getenv('MINIO_ROOT_USER', 'minio'),\n",
    "    'aws_secret_access_key': os.getenv('MINIO_ROOT_PASSWORD', 'minio123')\n",
    "}\n",
    "\n",
    "# Streaming configuration\n",
    "KAFKA_CONFIG = {\n",
    "    'bootstrap_servers': [os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')],\n",
    "    'schema_registry_url': os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "}\n",
    "\n",
    "# Service URLs\n",
    "SERVICE_URLS = {\n",
    "    'trino': os.getenv('TRINO_URL', 'http://trino:8080'),\n",
    "    'superset': os.getenv('SUPERSET_URL', 'http://superset:8088'),\n",
    "    'airflow': os.getenv('AIRFLOW_URL', 'http://airflow-apiserver:8080'),\n",
    "    'spark': os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded from environment variables\")\n",
    "print(f\"üóÑÔ∏è PostgreSQL: {POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}\")\n",
    "print(f\"üìä ClickHouse: {CLICKHOUSE_CONFIG['host']}:{CLICKHOUSE_CONFIG['port']}\")\n",
    "print(f\"‚òÅÔ∏è MinIO: {MINIO_CONFIG['endpoint_url']}\")\n",
    "print(f\"üì® Kafka: {KAFKA_CONFIG['bootstrap_servers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b94e7",
   "metadata": {},
   "source": [
    "## 1. PostgreSQL Connection Test\n",
    "\n",
    "Testing connection to the primary PostgreSQL database used for metadata and Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8509ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:06.824749Z",
     "iopub.status.busy": "2025-09-04T09:26:06.824382Z",
     "iopub.status.idle": "2025-09-04T09:26:06.845754Z",
     "shell.execute_reply": "2025-09-04T09:26:06.845133Z",
     "shell.execute_reply.started": "2025-09-04T09:26:06.824728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PostgreSQL Connection: SUCCESS\n",
      "üìä Version: PostgreSQL 16.10 (Debian 16.10-1.pgdg13+1) on x86_64-pc-linux-gnu\n",
      "üóÑÔ∏è Available Databases: postgres, metastore\n"
     ]
    }
   ],
   "source": [
    "def test_postgresql():\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(**POSTGRES_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Test query\n",
    "        cursor.execute(\"SELECT version();\")\n",
    "        version = cursor.fetchone()[0]\n",
    "        \n",
    "        # Get database list\n",
    "        cursor.execute(\"SELECT datname FROM pg_database WHERE datistemplate = false;\")\n",
    "        databases = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"‚úÖ PostgreSQL Connection: SUCCESS\")\n",
    "        print(f\"üìä Version: {version.split(',')[0]}\")\n",
    "        print(f\"üóÑÔ∏è Available Databases: {', '.join(databases)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PostgreSQL Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "postgres_status = test_postgresql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cda2f",
   "metadata": {},
   "source": [
    "## 2. ClickHouse Connection Test\n",
    "\n",
    "Testing connection to ClickHouse analytics database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bab6643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:08.911891Z",
     "iopub.status.busy": "2025-09-04T09:26:08.911508Z",
     "iopub.status.idle": "2025-09-04T09:26:09.001310Z",
     "shell.execute_reply": "2025-09-04T09:26:09.000183Z",
     "shell.execute_reply.started": "2025-09-04T09:26:08.911873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ClickHouse Connection: SUCCESS\n",
      "üìä Version: 25.8.1.5101\n",
      "üóÑÔ∏è Available Databases: INFORMATION_SCHEMA, analytics, default, information_schema, system\n",
      "üìù Test Records: 1\n"
     ]
    }
   ],
   "source": [
    "def test_clickhouse():\n",
    "    try:\n",
    "        # Connect to ClickHouse\n",
    "        client = clickhouse_connect.get_client(**CLICKHOUSE_CONFIG)\n",
    "        \n",
    "        # Test query\n",
    "        result = client.query(\"SELECT version()\")\n",
    "        version = result.result_rows[0][0]\n",
    "        \n",
    "        # Get databases\n",
    "        databases = client.query(\"SHOW DATABASES\")\n",
    "        db_list = [row[0] for row in databases.result_rows]\n",
    "        \n",
    "        # Test table creation and data insertion\n",
    "        client.command(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS test_table (\n",
    "                id UInt32,\n",
    "                name String,\n",
    "                timestamp DateTime\n",
    "            ) ENGINE = Memory\n",
    "        \"\"\")\n",
    "        \n",
    "        client.insert('test_table', [\n",
    "            [1, 'Test Record', datetime.now()]\n",
    "        ])\n",
    "        \n",
    "        count = client.query(\"SELECT count() FROM test_table\").result_rows[0][0]\n",
    "        \n",
    "        client.close()\n",
    "        \n",
    "        print(\"‚úÖ ClickHouse Connection: SUCCESS\")\n",
    "        print(f\"üìä Version: {version}\")\n",
    "        print(f\"üóÑÔ∏è Available Databases: {', '.join(db_list)}\")\n",
    "        print(f\"üìù Test Records: {count}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ClickHouse Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "clickhouse_status = test_clickhouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be07373",
   "metadata": {},
   "source": [
    "## 3. MinIO Object Storage Test\n",
    "\n",
    "Testing connection to MinIO S3-compatible object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad99468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:11.801036Z",
     "iopub.status.busy": "2025-09-04T09:26:11.800845Z",
     "iopub.status.idle": "2025-09-04T09:26:12.003246Z",
     "shell.execute_reply": "2025-09-04T09:26:12.002607Z",
     "shell.execute_reply.started": "2025-09-04T09:26:11.801023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MinIO Connection: SUCCESS\n",
      "ü™£ Available Buckets: None\n",
      "üìÅ Objects in test-bucket: 1\n",
      "üìÑ Test file uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "def test_minio():\n",
    "    try:\n",
    "        # Create S3 client\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=MINIO_CONFIG['endpoint_url'],\n",
    "            aws_access_key_id=MINIO_CONFIG['aws_access_key_id'],\n",
    "            aws_secret_access_key=MINIO_CONFIG['aws_secret_access_key']\n",
    "        )\n",
    "        \n",
    "        # List buckets\n",
    "        response = s3_client.list_buckets()\n",
    "        buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "        \n",
    "        # Create test bucket if it doesn't exist\n",
    "        test_bucket = 'test-bucket'\n",
    "        if test_bucket not in buckets:\n",
    "            s3_client.create_bucket(Bucket=test_bucket)\n",
    "        \n",
    "        # Upload test file\n",
    "        test_data = \"Hello from Data Forge! üöÄ\"\n",
    "        s3_client.put_object(\n",
    "            Bucket=test_bucket,\n",
    "            Key='test-file.txt',\n",
    "            Body=test_data.encode('utf-8')\n",
    "        )\n",
    "        \n",
    "        # List objects in test bucket\n",
    "        objects = s3_client.list_objects_v2(Bucket=test_bucket)\n",
    "        object_count = objects.get('KeyCount', 0)\n",
    "        \n",
    "        print(\"‚úÖ MinIO Connection: SUCCESS\")\n",
    "        print(f\"ü™£ Available Buckets: {', '.join(buckets) if buckets else 'None'}\")\n",
    "        print(f\"üìÅ Objects in test-bucket: {object_count}\")\n",
    "        print(f\"üìÑ Test file uploaded successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå MinIO Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "minio_status = test_minio()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a90aff",
   "metadata": {},
   "source": [
    "## 4. Kafka Messaging Test\n",
    "\n",
    "Testing Kafka producer and consumer functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4242b747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:14.929452Z",
     "iopub.status.busy": "2025-09-04T09:26:14.929027Z",
     "iopub.status.idle": "2025-09-04T09:26:15.885528Z",
     "shell.execute_reply": "2025-09-04T09:26:15.884350Z",
     "shell.execute_reply.started": "2025-09-04T09:26:14.929432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Kafka Connection: SUCCESS\n",
      "üì® Message sent to topic: test-topic\n",
      "üìç Partition: 0, Offset: 0\n",
      "üìã Schema Registry Status: 200\n"
     ]
    }
   ],
   "source": [
    "def test_kafka():\n",
    "    try:\n",
    "        # Test Kafka Producer\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=KAFKA_CONFIG['bootstrap_servers'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        \n",
    "        # Send test message\n",
    "        test_topic = 'test-topic'\n",
    "        test_message = {\n",
    "            'message': 'Hello from Data Forge!',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'source': 'jupyter-validation'\n",
    "        }\n",
    "        \n",
    "        future = producer.send(test_topic, test_message)\n",
    "        record_metadata = future.get(timeout=10)\n",
    "        producer.close()\n",
    "        \n",
    "        # Test Schema Registry\n",
    "        schema_response = requests.get(f\"{KAFKA_CONFIG['schema_registry_url']}/subjects\")\n",
    "        \n",
    "        print(\"‚úÖ Kafka Connection: SUCCESS\")\n",
    "        print(f\"üì® Message sent to topic: {test_topic}\")\n",
    "        print(f\"üìç Partition: {record_metadata.partition}, Offset: {record_metadata.offset}\")\n",
    "        print(f\"üìã Schema Registry Status: {schema_response.status_code}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Kafka Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "kafka_status = test_kafka()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac680c",
   "metadata": {},
   "source": [
    "## 5. Redis Cache Test\n",
    "\n",
    "Testing Redis connection and basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e53e1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:19.200500Z",
     "iopub.status.busy": "2025-09-04T09:26:19.200283Z",
     "iopub.status.idle": "2025-09-04T09:26:19.209437Z",
     "shell.execute_reply": "2025-09-04T09:26:19.208765Z",
     "shell.execute_reply.started": "2025-09-04T09:26:19.200486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Redis Connection: SUCCESS\n",
      "üèì Ping Response: True\n",
      "üìä Version: 7.2.10\n",
      "üíæ Used Memory: 982.02K\n",
      "üìù Test Value: Hello from Data Forge!\n"
     ]
    }
   ],
   "source": [
    "def test_redis():\n",
    "    try:\n",
    "        # Connect to Redis\n",
    "        r = redis.Redis(host='redis', port=6379, decode_responses=True)\n",
    "        \n",
    "        # Test connection\n",
    "        ping_response = r.ping()\n",
    "        \n",
    "        # Test basic operations\n",
    "        r.set('test:key', 'Hello from Data Forge!')\n",
    "        value = r.get('test:key')\n",
    "        \n",
    "        # Get Redis info\n",
    "        info = r.info()\n",
    "        redis_version = info['redis_version']\n",
    "        used_memory = info['used_memory_human']\n",
    "        \n",
    "        print(\"‚úÖ Redis Connection: SUCCESS\")\n",
    "        print(f\"üèì Ping Response: {ping_response}\")\n",
    "        print(f\"üìä Version: {redis_version}\")\n",
    "        print(f\"üíæ Used Memory: {used_memory}\")\n",
    "        print(f\"üìù Test Value: {value}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Redis Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "redis_status = test_redis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8adfd",
   "metadata": {},
   "source": [
    "## 6. Trino SQL Engine Test\n",
    "\n",
    "Testing Trino distributed SQL query engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "235c9f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:22.217497Z",
     "iopub.status.busy": "2025-09-04T09:26:22.217294Z",
     "iopub.status.idle": "2025-09-04T09:26:23.164098Z",
     "shell.execute_reply": "2025-09-04T09:26:23.163145Z",
     "shell.execute_reply.started": "2025-09-04T09:26:22.217483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trino Connection: SUCCESS\n",
      "üñ•Ô∏è Active Nodes: 1\n",
      "üìö Available Catalogs: clickhouse, iceberg, kafka, postgres, redis, system\n",
      "üïê Current Time: 2025-09-04 09:26:23.038000+00:00\n"
     ]
    }
   ],
   "source": [
    "def test_trino():\n",
    "    try:\n",
    "        # Connect to Trino\n",
    "        conn = trino_connect(\n",
    "            host='trino',\n",
    "            port=8080,\n",
    "            user='admin',\n",
    "            catalog='system',\n",
    "            schema='runtime'\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Test basic query\n",
    "        cursor.execute(\"SELECT node_version FROM system.runtime.nodes\")\n",
    "        nodes = cursor.fetchall()\n",
    "        \n",
    "        # Get catalogs\n",
    "        cursor.execute(\"SHOW CATALOGS\")\n",
    "        catalogs = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        # Test sample query\n",
    "        cursor.execute(\"SELECT current_timestamp\")\n",
    "        current_time = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        print(\"‚úÖ Trino Connection: SUCCESS\")\n",
    "        print(f\"üñ•Ô∏è Active Nodes: {len(nodes)}\")\n",
    "        print(f\"üìö Available Catalogs: {', '.join(catalogs)}\")\n",
    "        print(f\"üïê Current Time: {current_time}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Trino Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "trino_status = test_trino()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209f4f6",
   "metadata": {},
   "source": [
    "## 7. Spark Distributed Computing Test\n",
    "\n",
    "Testing Apache Spark connection and basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02568d17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:25.738358Z",
     "iopub.status.busy": "2025-09-04T09:26:25.738148Z",
     "iopub.status.idle": "2025-09-04T09:26:33.395074Z",
     "shell.execute_reply": "2025-09-04T09:26:33.394054Z",
     "shell.execute_reply.started": "2025-09-04T09:26:25.738344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Connection: SUCCESS\n",
      "üÜî Application ID: app-20250904092628-0006\n",
      "‚ö° Default Parallelism: 2\n",
      "üìä Test Result Count: 2\n",
      "üë• Filtered Users: ['Bob', 'Charlie']\n"
     ]
    }
   ],
   "source": [
    "def test_spark():\n",
    "    try:\n",
    "        # Create Spark Session\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"DataForgeValidation\") \\\n",
    "            .master(SERVICE_URLS['spark']) \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Test basic operations\n",
    "        data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\n",
    "        columns = [\"id\", \"name\", \"age\"]\n",
    "        \n",
    "        df = spark.createDataFrame(data, columns)\n",
    "        \n",
    "        # Perform transformations\n",
    "        result = df.filter(df.age > 25).select(\"name\", \"age\").collect()\n",
    "        \n",
    "        # Get Spark context info\n",
    "        sc = spark.sparkContext\n",
    "        app_id = sc.applicationId\n",
    "        default_parallelism = sc.defaultParallelism\n",
    "        \n",
    "        spark.stop()\n",
    "        \n",
    "        print(\"‚úÖ Spark Connection: SUCCESS\")\n",
    "        print(f\"üÜî Application ID: {app_id}\")\n",
    "        print(f\"‚ö° Default Parallelism: {default_parallelism}\")\n",
    "        print(f\"üìä Test Result Count: {len(result)}\")\n",
    "        print(f\"üë• Filtered Users: {[row.name for row in result]}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Spark Connection: FAILED\")\n",
    "        print(f\"üö® Error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "spark_status = test_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ee00a",
   "metadata": {},
   "source": [
    "## 8. Service Health Checks\n",
    "\n",
    "Testing HTTP endpoints for web services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3c6242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:39.473123Z",
     "iopub.status.busy": "2025-09-04T09:26:39.472910Z",
     "iopub.status.idle": "2025-09-04T09:26:42.321655Z",
     "shell.execute_reply": "2025-09-04T09:26:42.321093Z",
     "shell.execute_reply.started": "2025-09-04T09:26:39.473105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê HTTP Services Health Check:\n",
      "‚úÖ Trino: SUCCESS\n",
      "‚úÖ Superset: SUCCESS\n",
      "‚ùå Airflow: FAILED\n",
      "   üö® Error: HTTPConnectionPool(host='airflow-apiserver', port=8080): Max retries exceeded with url: /health (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x7248567facd0>: Failed to resolve 'airflow-apiserver' ([Errno -2] Name or service not known)\"))\n"
     ]
    }
   ],
   "source": [
    "def test_http_services():\n",
    "    services_status = {}\n",
    "    \n",
    "    # Test each service\n",
    "    for service_name, url in SERVICE_URLS.items():\n",
    "        try:\n",
    "            if service_name == 'spark':  # Skip Spark master web UI for now\n",
    "                continue\n",
    "                \n",
    "            # Adjust URLs for HTTP requests\n",
    "            if service_name == 'trino':\n",
    "                test_url = f\"{url}/v1/info\"\n",
    "            elif service_name == 'airflow':\n",
    "                test_url = f\"{url}/health\"\n",
    "            elif service_name == 'superset':\n",
    "                test_url = f\"{url}/health\"\n",
    "            else:\n",
    "                test_url = url\n",
    "            \n",
    "            response = requests.get(test_url, timeout=10)\n",
    "            services_status[service_name] = {\n",
    "                'status': 'SUCCESS' if response.status_code == 200 else f'HTTP {response.status_code}',\n",
    "                'url': test_url\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            services_status[service_name] = {\n",
    "                'status': 'FAILED',\n",
    "                'error': str(e),\n",
    "                'url': test_url if 'test_url' in locals() else url\n",
    "            }\n",
    "    \n",
    "    print(\"üåê HTTP Services Health Check:\")\n",
    "    for service, status in services_status.items():\n",
    "        emoji = \"‚úÖ\" if status['status'] == 'SUCCESS' else \"‚ùå\"\n",
    "        print(f\"{emoji} {service.title()}: {status['status']}\")\n",
    "        if 'error' in status:\n",
    "            print(f\"   üö® Error: {status['error']}\")\n",
    "    \n",
    "    return services_status\n",
    "\n",
    "http_services_status = test_http_services()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a8277",
   "metadata": {},
   "source": [
    "## üìä Connection Summary Report\n",
    "\n",
    "Overview of all service connection tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922b986c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:26:50.589325Z",
     "iopub.status.busy": "2025-09-04T09:26:50.588917Z",
     "iopub.status.idle": "2025-09-04T09:26:50.595029Z",
     "shell.execute_reply": "2025-09-04T09:26:50.594358Z",
     "shell.execute_reply.started": "2025-09-04T09:26:50.589302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ DATA FORGE SERVICES CONNECTION SUMMARY\n",
      "============================================================\n",
      "‚úÖ PostgreSQL                CONNECTED\n",
      "‚úÖ ClickHouse                CONNECTED\n",
      "‚úÖ MinIO                     CONNECTED\n",
      "‚úÖ Kafka                     CONNECTED\n",
      "‚úÖ Redis                     CONNECTED\n",
      "‚úÖ Trino                     CONNECTED\n",
      "‚úÖ Spark                     CONNECTED\n",
      "‚úÖ Trino (HTTP)              CONNECTED\n",
      "‚úÖ Superset (HTTP)           CONNECTED\n",
      "‚ùå Airflow (HTTP)            FAILED\n",
      "============================================================\n",
      "üìà Success Rate: 9/10 (90.0%)\n",
      "üïê Validation completed at: 2025-09-04 09:26:50.591864\n",
      "üü° Most services are operational. Check failed connections.\n"
     ]
    }
   ],
   "source": [
    "# Create summary report\n",
    "services_summary = {\n",
    "    'PostgreSQL': postgres_status,\n",
    "    'ClickHouse': clickhouse_status,\n",
    "    'MinIO': minio_status,\n",
    "    'Kafka': kafka_status,\n",
    "    'Redis': redis_status,\n",
    "    'Trino': trino_status,\n",
    "    'Spark': spark_status\n",
    "}\n",
    "\n",
    "# Add HTTP services\n",
    "for service, status in http_services_status.items():\n",
    "    services_summary[f\"{service.title()} (HTTP)\"] = status['status'] == 'SUCCESS'\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "print(\"üéØ DATA FORGE SERVICES CONNECTION SUMMARY\")\n",
    "print(\"\" + \"=\"*60)\n",
    "\n",
    "successful_connections = 0\n",
    "total_connections = len(services_summary)\n",
    "\n",
    "for service, is_successful in services_summary.items():\n",
    "    status_emoji = \"‚úÖ\" if is_successful else \"‚ùå\"\n",
    "    status_text = \"CONNECTED\" if is_successful else \"FAILED\"\n",
    "    print(f\"{status_emoji} {service:<25} {status_text}\")\n",
    "    if is_successful:\n",
    "        successful_connections += 1\n",
    "\n",
    "print(\"\" + \"=\"*60)\n",
    "success_rate = (successful_connections / total_connections) * 100\n",
    "print(f\"üìà Success Rate: {successful_connections}/{total_connections} ({success_rate:.1f}%)\")\n",
    "print(f\"üïê Validation completed at: {datetime.now()}\")\n",
    "\n",
    "if success_rate == 100:\n",
    "    print(\"üéâ All services are connected and operational!\")\n",
    "elif success_rate >= 80:\n",
    "    print(\"üü° Most services are operational. Check failed connections.\")\n",
    "else:\n",
    "    print(\"üî¥ Multiple service connection issues detected. Please check service status.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf3861",
   "metadata": {},
   "source": [
    "## üîß Troubleshooting Guide\n",
    "\n",
    "If any connections failed, here are common solutions:\n",
    "\n",
    "### PostgreSQL Issues\n",
    "- Ensure PostgreSQL container is running: `docker compose ps postgres`\n",
    "- Check logs: `docker compose logs postgres`\n",
    "\n",
    "### ClickHouse Issues\n",
    "- Verify ClickHouse is healthy: `docker compose ps clickhouse`\n",
    "- Check configuration: `docker compose logs clickhouse`\n",
    "\n",
    "### MinIO Issues\n",
    "- Check MinIO service: `docker compose ps minio`\n",
    "- Verify credentials in `.env` file\n",
    "\n",
    "### Kafka Issues\n",
    "- Ensure Kafka is running: `docker compose ps kafka`\n",
    "- Check broker logs: `docker compose logs kafka`\n",
    "\n",
    "### Spark Issues\n",
    "- Check Spark master: `docker compose ps spark-master`\n",
    "- Verify workers: `docker compose ps | grep spark-worker`\n",
    "\n",
    "### General Issues\n",
    "- Restart services: `docker compose --profile core --profile explore restart`\n",
    "- Check all services: `docker compose ps`\n",
    "- View logs: `docker compose logs [service-name]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
