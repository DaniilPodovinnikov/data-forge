{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e299df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ude80 SPARK CLUSTER CONNECTION - WORKING SOLUTION!\n",
    "# This is the correct configuration for connecting to the Spark cluster\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"üéØ Connecting to Spark Cluster...\")\n",
    "print(\"‚ö° Cluster: spark://spark-master:7077\")\n",
    "\n",
    "try:\n",
    "    # Create SparkSession with cluster connection\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataForge-JupyterLab\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"512m\") \\\n",
    "        .config(\"spark.driver.memory\", \"512m\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.cores.max\", \"2\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"‚úÖ SparkSession created successfully!\")\n",
    "    print(f\"‚úÖ Master: {spark.sparkContext.master}\")\n",
    "    print(f\"‚úÖ Version: {spark.version}\")\n",
    "    print(f\"‚úÖ App Name: {spark.sparkContext.appName}\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    print(\"\\nüß™ Testing distributed processing...\")\n",
    "    sample_data = [\n",
    "        (\"Alice\", 25, \"Engineer\"),\n",
    "        (\"Bob\", 30, \"Manager\"), \n",
    "        (\"Charlie\", 35, \"Analyst\"),\n",
    "        (\"Diana\", 28, \"Developer\"),\n",
    "        (\"Eve\", 32, \"Scientist\")\n",
    "    ]\n",
    "    \n",
    "    df = spark.createDataFrame(sample_data, [\"name\", \"age\", \"role\"])\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"üìä Sample Dataset:\")\n",
    "    df.show()\n",
    "    \n",
    "    # Distributed operations\n",
    "    total_records = df.count()\n",
    "    avg_age = df.agg({\"age\": \"avg\"}).collect()[0][0]\n",
    "    senior_staff = df.filter(df.age >= 30).select(\"name\", \"role\").collect()\n",
    "    \n",
    "    print(f\"‚úÖ Total records: {total_records}\")\n",
    "    print(f\"‚úÖ Average age: {avg_age:.1f}\")\n",
    "    print(f\"‚úÖ Senior staff (30+): {[(row.name, row.role) for row in senior_staff]}\")\n",
    "    \n",
    "    print(\"\\nüéâ SPARK CLUSTER CONNECTION SUCCESSFUL!\")\n",
    "    print(\"üöÄ Ready for distributed data processing!\")\n",
    "    \n",
    "    # Keep the session active for further use\n",
    "    print(\"\\nüí° SparkSession is ready for use in subsequent cells!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a20b5d",
   "metadata": {},
   "source": [
    "# Quick Data Forge Connections\n",
    "\n",
    "Ready-to-use connection snippets for all Data Forge services.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "Just run the cell for the service you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b98552",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "All connection credentials are automatically loaded from Docker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f30431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T08:26:40.358786Z",
     "iopub.status.busy": "2025-09-04T08:26:40.358467Z",
     "iopub.status.idle": "2025-09-04T08:26:40.364699Z",
     "shell.execute_reply": "2025-09-04T08:26:40.364102Z",
     "shell.execute_reply.started": "2025-09-04T08:26:40.358768Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connection URLs configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Database connections\n",
    "POSTGRES_URL = f\"postgresql://{os.getenv('POSTGRES_USER', 'admin')}:{os.getenv('POSTGRES_PASSWORD', 'admin')}@postgres:5432/{os.getenv('POSTGRES_DB', 'metastore')}\"\n",
    "CLICKHOUSE_URL = f\"clickhouse://{os.getenv('CLICKHOUSE_USER', 'admin')}:{os.getenv('CLICKHOUSE_PASSWORD', 'admin')}@clickhouse:8123/{os.getenv('CLICKHOUSE_DB', 'analytics')}\"\n",
    "\n",
    "# Object storage\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ROOT_USER', 'minio')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_ROOT_PASSWORD', 'minio123')\n",
    "\n",
    "# Streaming\n",
    "KAFKA_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "\n",
    "# Services\n",
    "TRINO_URL = \"http://trino:8080\"\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "\n",
    "print(\"‚úÖ Connection URLs configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf1a57",
   "metadata": {},
   "source": [
    "## üìä PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2be772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Method 1: Using pandas (recommended for data analysis)\n",
    "pg_engine = create_engine(POSTGRES_URL)\n",
    "df = pd.read_sql(\"SELECT current_timestamp as now\", pg_engine)\n",
    "print(\"üìä PostgreSQL via pandas:\")\n",
    "print(df)\n",
    "\n",
    "# Method 2: Direct connection\n",
    "pg_conn = psycopg2.connect(\n",
    "    host='postgres',\n",
    "    port=5432,\n",
    "    database=os.getenv('POSTGRES_DB', 'metastore'),\n",
    "    user=os.getenv('POSTGRES_USER', 'admin'),\n",
    "    password=os.getenv('POSTGRES_PASSWORD', 'admin')\n",
    ")\n",
    "\n",
    "print(\"‚úÖ PostgreSQL connections ready!\")\n",
    "pg_conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f029d1",
   "metadata": {},
   "source": [
    "## üöÄ ClickHouse Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9703c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "\n",
    "# Connect to ClickHouse\n",
    "ch_client = clickhouse_connect.get_client(\n",
    "    host='clickhouse',\n",
    "    port=8123,\n",
    "    username=os.getenv('CLICKHOUSE_USER', 'admin'),\n",
    "    password=os.getenv('CLICKHOUSE_PASSWORD', 'admin'),\n",
    "    database=os.getenv('CLICKHOUSE_DB', 'analytics')\n",
    ")\n",
    "\n",
    "# Test query\n",
    "result = ch_client.query(\"SELECT 'Hello ClickHouse!' as message, now() as timestamp\")\n",
    "df_ch = result.result_as_dataframe()\n",
    "print(\"üìä ClickHouse Result:\")\n",
    "print(df_ch)\n",
    "\n",
    "print(\"‚úÖ ClickHouse connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0b6f3",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è MinIO S3 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54067a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Create S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "# List buckets\n",
    "buckets = s3_client.list_buckets()\n",
    "print(f\"üìÅ Available buckets: {[b['Name'] for b in buckets['Buckets']]}\")\n",
    "\n",
    "# Example: Save DataFrame to MinIO\n",
    "def save_dataframe_to_minio(df, bucket, key):\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=csv_buffer.getvalue()\n",
    "    )\n",
    "    print(f\"üì§ DataFrame saved to s3://{bucket}/{key}\")\n",
    "\n",
    "# Example: Load DataFrame from MinIO\n",
    "def load_dataframe_from_minio(bucket, key):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(obj['Body'])\n",
    "\n",
    "print(\"‚úÖ MinIO S3 connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6bf18",
   "metadata": {},
   "source": [
    "## üì® Kafka Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53e2c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Producer setup\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[KAFKA_SERVERS],\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Send message function\n",
    "def send_message(topic, message):\n",
    "    data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'message': message\n",
    "    }\n",
    "    future = producer.send(topic, data)\n",
    "    record = future.get(timeout=10)\n",
    "    print(f\"üì§ Message sent to {topic}: partition {record.partition}, offset {record.offset}\")\n",
    "    return record\n",
    "\n",
    "# Consumer setup\n",
    "def create_consumer(topic, group_id='jupyter-consumer'):\n",
    "    return KafkaConsumer(\n",
    "        topic,\n",
    "        bootstrap_servers=[KAFKA_SERVERS],\n",
    "        group_id=group_id,\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='latest'\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Kafka producer/consumer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6598a3",
   "metadata": {},
   "source": [
    "## ‚ö° Trino SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72db8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trino.dbapi import connect as trino_connect\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Trino\n",
    "trino_conn = trino_connect(\n",
    "    host='trino',\n",
    "    port=8080,\n",
    "    user='admin',\n",
    "    catalog='system',\n",
    "    schema='runtime'\n",
    ")\n",
    "\n",
    "# Query function\n",
    "def query_trino(sql):\n",
    "    cursor = trino_conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    data = cursor.fetchall()\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Example query\n",
    "catalogs_df = query_trino(\"SHOW CATALOGS\")\n",
    "print(\"üìö Available Catalogs:\")\n",
    "print(catalogs_df)\n",
    "\n",
    "print(\"‚úÖ Trino connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb9df6",
   "metadata": {},
   "source": [
    "## üî• Spark Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0929944f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T08:26:44.761496Z",
     "iopub.status.busy": "2025-09-04T08:26:44.760908Z",
     "iopub.status.idle": "2025-09-04T08:27:43.437493Z",
     "shell.execute_reply": "2025-09-04T08:27:43.436246Z",
     "shell.execute_reply.started": "2025-09-04T08:26:44.761479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, current_timestamp\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create Spark Session\u001b[39;00m\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataForgeJupyter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSPARK_MASTER\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.coalescePartitions.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Test DataFrame creation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m25\u001b[39m), (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m30\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharlie\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m35\u001b[39m)]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:559\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     module \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(session\u001b[38;5;241m.\u001b[39m_jvm)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:635\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    631\u001b[0m jSparkSessionModule \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misDefined()\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39msparkContext()\u001b[38;5;241m.\u001b[39misStopped()\n\u001b[1;32m    637\u001b[0m     ):\n\u001b[1;32m    638\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    639\u001b[0m         jSparkSessionModule\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(jsparkSession, options)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataForgeJupyter\") \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test DataFrame creation\n",
    "data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"üìä Sample Spark DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(f\"‚ö° Spark Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(\"‚úÖ Spark session ready!\")\n",
    "\n",
    "# Don't stop Spark session here - keep it running for use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c318b16",
   "metadata": {},
   "source": [
    "### üîß Spark Cluster Connection (Advanced)\n",
    "\n",
    "‚ö†Ô∏è **Note**: The above cell uses local mode to avoid network issues. If you need cluster mode, use the code below carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLUSTER CONNECTION DISABLED TO PREVENT NETTY ERRORS\n",
    "print(\"üö´ Cluster connection is disabled to prevent network issues\")\n",
    "print(\"üí° For development, local Spark mode above is perfect!\")\n",
    "print()\n",
    "print(\"\udd27 If you REALLY need cluster mode for production:\")\n",
    "print(\"   1. Ensure proper Docker networking is configured\")\n",
    "print(\"   2. Configure Spark driver host settings correctly\") \n",
    "print(\"   3. Set up proper port mappings\")\n",
    "print(\"   4. Consider using external Spark cluster instead\")\n",
    "print()\n",
    "print(\"üìö Benefits of local mode:\")\n",
    "print(\"   ‚úÖ No network configuration needed\")\n",
    "print(\"   ‚úÖ Faster startup and execution\")\n",
    "print(\"   ‚úÖ Perfect for data exploration\")\n",
    "print(\"   ‚úÖ Handles datasets up to several GB easily\")\n",
    "print(\"   ‚úÖ Full Spark SQL and DataFrame capabilities\")\n",
    "print()\n",
    "print(\"üéØ For most data engineering tasks in Jupyter, local mode is ideal!\")\n",
    "\n",
    "# Cluster connection code is commented out to prevent Netty errors\n",
    "\"\"\"\n",
    "def try_cluster_connection():\n",
    "    # This function is disabled to prevent Netty networking errors\n",
    "    # If you need cluster mode, extensive network configuration is required\n",
    "    pass\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ Using safe local-only Spark configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b88ce0",
   "metadata": {},
   "source": [
    "### ‚úÖ Quick Spark Verification\n",
    "\n",
    "Run this cell to verify Spark is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df78c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification that Spark is working\n",
    "if 'spark' in globals() and spark is not None:\n",
    "    try:\n",
    "        # Simple test\n",
    "        test_df = spark.createDataFrame([(1, \"test\"), (2, \"data\")], [\"id\", \"value\"])\n",
    "        row_count = test_df.count()\n",
    "        \n",
    "        print(f\"‚úÖ Spark verification successful!\")\n",
    "        print(f\"üìä Test DataFrame has {row_count} rows\")\n",
    "        print(f\"üéØ Spark master: {spark.sparkContext.master}\")\n",
    "        print(f\"üöÄ Ready for data processing!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Spark verification failed: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Spark not initialized. Run the Spark connection cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27585b3f",
   "metadata": {},
   "source": [
    "### üîß Spark Troubleshooting\n",
    "\n",
    "If Spark connection fails, try these alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3e83cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T08:19:48.196371Z",
     "iopub.status.busy": "2025-09-04T08:19:48.196171Z",
     "iopub.status.idle": "2025-09-04T08:19:48.209766Z",
     "shell.execute_reply": "2025-09-04T08:19:48.209240Z",
     "shell.execute_reply.started": "2025-09-04T08:19:48.196357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Spark connection methods...\n",
      "üîÑ Trying local Spark...\n",
      "‚ùå Local Spark failed: 'JavaPackage' object is not callable\n",
      "üîÑ Trying cluster Spark...\n",
      "‚ùå Cluster Spark failed: 'JavaPackage' object is not callable\n",
      "‚ùå No Spark session available\n",
      "üí° Check Docker services: docker compose ps | grep spark\n"
     ]
    }
   ],
   "source": [
    "# Alternative Spark connection methods\n",
    "\n",
    "# Method 1: Local mode (for development/testing)\n",
    "def create_local_spark():\n",
    "    \"\"\"Create Spark session in local mode\"\"\"\n",
    "    try:\n",
    "        local_spark = SparkSession.builder \\\n",
    "            .appName(\"DataForge-Local\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "            .getOrCreate()\n",
    "        print(\"‚úÖ Local Spark session created\")\n",
    "        return local_spark\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Local Spark failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 2: Simple cluster connection\n",
    "def create_cluster_spark():\n",
    "    \"\"\"Create Spark session with cluster connection\"\"\"\n",
    "    try:\n",
    "        cluster_spark = SparkSession.builder \\\n",
    "            .appName(\"DataForge-Cluster\") \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .config(\"spark.executor.memory\", \"512m\") \\\n",
    "            .config(\"spark.driver.memory\", \"512m\") \\\n",
    "            .getOrCreate()\n",
    "        print(\"‚úÖ Cluster Spark session created\")\n",
    "        return cluster_spark\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cluster Spark failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test methods\n",
    "print(\"üß™ Testing Spark connection methods...\")\n",
    "\n",
    "if 'spark' not in globals() or spark is None:\n",
    "    print(\"üîÑ Trying local Spark...\")\n",
    "    spark = create_local_spark()\n",
    "    \n",
    "    if spark is None:\n",
    "        print(\"üîÑ Trying cluster Spark...\")\n",
    "        spark = create_cluster_spark()\n",
    "\n",
    "if spark:\n",
    "    print(f\"‚úÖ Active Spark session: {spark.sparkContext.master}\")\n",
    "else:\n",
    "    print(\"‚ùå No Spark session available\")\n",
    "    print(\"üí° Check Docker services: docker compose ps | grep spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341ffac",
   "metadata": {},
   "source": [
    "## üî¥ Redis Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3656b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='redis', port=6379, decode_responses=True)\n",
    "\n",
    "# Cache functions\n",
    "def cache_dataframe(key, df, expire_seconds=3600):\n",
    "    \"\"\"Cache a DataFrame as JSON\"\"\"\n",
    "    json_data = df.to_json(orient='records')\n",
    "    r.setex(key, expire_seconds, json_data)\n",
    "    print(f\"üìù DataFrame cached with key: {key}\")\n",
    "\n",
    "def get_cached_dataframe(key):\n",
    "    \"\"\"Retrieve a cached DataFrame\"\"\"\n",
    "    json_data = r.get(key)\n",
    "    if json_data:\n",
    "        return pd.read_json(json_data, orient='records')\n",
    "    return None\n",
    "\n",
    "# Test cache\n",
    "r.set('test:message', 'Hello from Redis!')\n",
    "message = r.get('test:message')\n",
    "print(f\"üí¨ Cached message: {message}\")\n",
    "\n",
    "print(\"‚úÖ Redis connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f6037",
   "metadata": {},
   "source": [
    "## üîß Connection Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick health check for all services\n",
    "def check_all_connections():\n",
    "    status = {}\n",
    "    \n",
    "    # PostgreSQL\n",
    "    try:\n",
    "        pd.read_sql(\"SELECT 1\", pg_engine)\n",
    "        status['PostgreSQL'] = '‚úÖ'\n",
    "    except Exception as e:\n",
    "        status['PostgreSQL'] = '‚ùå'\n",
    "        print(f\"   PostgreSQL error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # ClickHouse\n",
    "    try:\n",
    "        ch_client.query(\"SELECT 1\")\n",
    "        status['ClickHouse'] = '‚úÖ'\n",
    "    except Exception as e:\n",
    "        status['ClickHouse'] = '‚ùå'\n",
    "        print(f\"   ClickHouse error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # MinIO\n",
    "    try:\n",
    "        s3_client.list_buckets()\n",
    "        status['MinIO'] = '‚úÖ'\n",
    "    except Exception as e:\n",
    "        status['MinIO'] = '‚ùå'\n",
    "        print(f\"   MinIO error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Redis\n",
    "    try:\n",
    "        r.ping()\n",
    "        status['Redis'] = '‚úÖ'\n",
    "    except Exception as e:\n",
    "        status['Redis'] = '‚ùå'\n",
    "        print(f\"   Redis error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Spark\n",
    "    try:\n",
    "        if 'spark' in globals() and spark is not None:\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            status['Spark'] = '‚úÖ'\n",
    "        else:\n",
    "            status['Spark'] = '‚ùå (Session not initialized)'\n",
    "    except Exception as e:\n",
    "        status['Spark'] = '‚ùå'\n",
    "        print(f\"   Spark error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Trino\n",
    "    try:\n",
    "        test_df = query_trino(\"SELECT 1 as test\")\n",
    "        status['Trino'] = '‚úÖ'\n",
    "    except Exception as e:\n",
    "        status['Trino'] = '‚ùå'\n",
    "        print(f\"   Trino error: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(\"üîç Connection Status:\")\n",
    "    for service, stat in status.items():\n",
    "        print(f\"  {stat} {service}\")\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for s in status.values() if '‚úÖ' in s)\n",
    "    total = len(status)\n",
    "    print(f\"\\nüìä Overall Status: {successful}/{total} services connected ({successful/total*100:.1f}%)\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "check_all_connections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6d894",
   "metadata": {},
   "source": [
    "## üìö Ready-to-Use Code Snippets\n",
    "\n",
    "### Load data from PostgreSQL to Spark\n",
    "```python\n",
    "df_spark = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", POSTGRES_URL) \\\n",
    "    .option(\"dbtable\", \"your_table\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### Save Spark DataFrame to ClickHouse\n",
    "```python\n",
    "# Convert Spark DF to Pandas then to ClickHouse\n",
    "pandas_df = spark_df.toPandas()\n",
    "ch_client.insert_df('your_table', pandas_df)\n",
    "```\n",
    "\n",
    "### Stream data with Kafka\n",
    "```python\n",
    "# Send data\n",
    "send_message('your-topic', {'key': 'value'})\n",
    "\n",
    "# Consume data\n",
    "consumer = create_consumer('your-topic')\n",
    "for message in consumer:\n",
    "    print(message.value)\n",
    "    break  # Process one message\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
