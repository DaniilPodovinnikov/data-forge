{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Data Forge Connections\n",
    "\n",
    "Ready-to-use connection snippets for all Data Forge services.\n",
    "\n",
    "## Quick Start\n",
    "Just run the cell for the service you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "**Run this cell first!** All connection configurations are loaded from Docker environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "POSTGRES_URL = f\"postgresql://{os.getenv('POSTGRES_USER', 'admin')}:{os.getenv('POSTGRES_PASSWORD', 'admin')}@postgres:5432/{os.getenv('POSTGRES_DB', 'metastore')}\"\n",
    "CLICKHOUSE_URL = f\"clickhouse://{os.getenv('CLICKHOUSE_USER', 'admin')}:{os.getenv('CLICKHOUSE_PASSWORD', 'admin')}@clickhouse:8123/{os.getenv('CLICKHOUSE_DB', 'analytics')}\"\n",
    "\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = os.getenv('MINIO_ROOT_USER', 'minio')\n",
    "MINIO_SECRET_KEY = os.getenv('MINIO_ROOT_PASSWORD', 'minio123')\n",
    "\n",
    "KAFKA_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')\n",
    "SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://schema-registry:8081')\n",
    "\n",
    "TRINO_URL = \"http://trino:8080\"\n",
    "SPARK_MASTER = os.getenv('SPARK_MASTER_URL', 'spark://spark-master:7077')\n",
    "\n",
    "print(\"[SUCCESS] Connection URLs configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"Connecting to Spark Cluster...\")\n",
    "print(f\"Cluster: {SPARK_MASTER}\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"DataForge-JupyterLab\") \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"spark.executor.memory\", \"512m\") \\\n",
    "        .config(\"spark.driver.memory\", \"512m\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.cores.max\", \"2\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"[SUCCESS] SparkSession created successfully!\")\n",
    "    print(f\"[SUCCESS] Master: {spark.sparkContext.master}\")\n",
    "    print(f\"[SUCCESS] Version: {spark.version}\")\n",
    "    sample_data = [\n",
    "        (\"Alice\", 25, \"Engineer\"),\n",
    "        (\"Bob\", 30, \"Manager\"), \n",
    "        (\"Charlie\", 35, \"Analyst\")\n",
    "    ]\n",
    "    \n",
    "    df = spark.createDataFrame(sample_data, [\"name\", \"age\", \"role\"])\n",
    "    print(\"Sample Dataset:\")\n",
    "    df.show()\n",
    "    \n",
    "    print(\"\\nSPARK CLUSTER CONNECTION SUCCESSFUL!\")\n",
    "    print(\"SparkSession is ready for use!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Connection failed: {e}\")\n",
    "    print(\"Make sure to run the Environment Setup cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Using pandas (recommended)\n",
    "pg_engine = create_engine(POSTGRES_URL)\n",
    "df = pd.read_sql(\"SELECT current_timestamp as now\", pg_engine)\n",
    "print(\"PostgreSQL Result:\")\n",
    "print(df)\n",
    "\n",
    "print(\"[SUCCESS] PostgreSQL connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClickHouse Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "import pandas as pd\n",
    "\n",
    "ch_client = clickhouse_connect.get_client(\n",
    "    host='clickhouse',\n",
    "    port=8123,\n",
    "    username=os.getenv('CLICKHOUSE_USER', 'admin'),\n",
    "    password=os.getenv('CLICKHOUSE_PASSWORD', 'admin'),\n",
    "    database=os.getenv('CLICKHOUSE_DB', 'analytics')\n",
    ")\n",
    "\n",
    "# Query with DataFrame result\n",
    "result = ch_client.query(\"SELECT 'Hello ClickHouse!' as message, now() as timestamp\")\n",
    "# Convert to DataFrame using the correct method\n",
    "df_ch = pd.DataFrame(result.result_rows, columns=result.column_names)\n",
    "print(\"ClickHouse Result:\")\n",
    "print(df_ch)\n",
    "\n",
    "print(\"[SUCCESS] ClickHouse connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinIO S3 Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY\n",
    ")\n",
    "\n",
    "buckets = s3_client.list_buckets()\n",
    "print(f\"Available buckets: {[b['Name'] for b in buckets['Buckets']]}\")\n",
    "\n",
    "print(\"[SUCCESS] MinIO S3 connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trino SQL Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trino.dbapi import connect as trino_connect\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to Trino\n",
    "trino_conn = trino_connect(\n",
    "    host='trino',\n",
    "    port=8080,\n",
    "    user='admin',\n",
    "    catalog='system',\n",
    "    schema='runtime'\n",
    ")\n",
    "\n",
    "# Query function\n",
    "def query_trino(sql):\n",
    "    cursor = trino_conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    data = cursor.fetchall()\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Example query\n",
    "catalogs_df = query_trino(\"SHOW CATALOGS\")\n",
    "print(\"Available Catalogs:\")\n",
    "print(catalogs_df)\n",
    "\n",
    "print(\"[SUCCESS] Trino connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redis Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "# Connect to Redis\n",
    "r = redis.Redis(host='redis', port=6379, decode_responses=True)\n",
    "\n",
    "# Cache functions\n",
    "def cache_dataframe(key, df, expire_seconds=3600):\n",
    "    \"\"\"Cache a DataFrame as JSON\"\"\"\n",
    "    json_data = df.to_json(orient='records')\n",
    "    r.setex(key, expire_seconds, json_data)\n",
    "    print(f\"DataFrame cached with key: {key}\")\n",
    "\n",
    "def get_cached_dataframe(key):\n",
    "    \"\"\"Retrieve a cached DataFrame\"\"\"\n",
    "    json_data = r.get(key)\n",
    "    if json_data:\n",
    "        return pd.read_json(json_data, orient='records')\n",
    "    return None\n",
    "\n",
    "# Test cache\n",
    "r.set('test:message', 'Hello from Redis!')\n",
    "message = r.get('test:message')\n",
    "print(f\"Cached message: {message}\")\n",
    "\n",
    "print(\"[SUCCESS] Redis connection ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Producer setup\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[KAFKA_SERVERS],\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Send message function\n",
    "def send_message(topic, message):\n",
    "    data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'message': message\n",
    "    }\n",
    "    future = producer.send(topic, data)\n",
    "    record = future.get(timeout=10)\n",
    "    print(f\"Message sent to {topic}: partition {record.partition}, offset {record.offset}\")\n",
    "    return record\n",
    "\n",
    "# Consumer setup\n",
    "def create_consumer(topic, group_id='jupyter-consumer'):\n",
    "    return KafkaConsumer(\n",
    "        topic,\n",
    "        bootstrap_servers=[KAFKA_SERVERS],\n",
    "        group_id=group_id,\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='latest'\n",
    "    )\n",
    "\n",
    "print(\"[SUCCESS] Kafka producer/consumer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_all_connections():\n",
    "    status = {}\n",
    "    \n",
    "    try:\n",
    "        pd.read_sql(\"SELECT 1\", pg_engine)\n",
    "        status['PostgreSQL'] = '[SUCCESS]'\n",
    "    except:\n",
    "        status['PostgreSQL'] = '[ERROR]'\n",
    "    \n",
    "    try:\n",
    "        ch_client.query(\"SELECT 1\")\n",
    "        status['ClickHouse'] = '[SUCCESS]'\n",
    "    except:\n",
    "        status['ClickHouse'] = '[ERROR]'\n",
    "    \n",
    "    try:\n",
    "        s3_client.list_buckets()\n",
    "        status['MinIO'] = '[SUCCESS]'\n",
    "    except:\n",
    "        status['MinIO'] = '[ERROR]'\n",
    "    \n",
    "    try:\n",
    "        if 'spark' in globals() and spark is not None:\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            status['Spark'] = '[SUCCESS]'\n",
    "        else:\n",
    "            status['Spark'] = '[ERROR] (Not initialized)'\n",
    "    except:\n",
    "        status['Spark'] = '[ERROR]'\n",
    "    \n",
    "    print(\"Connection Status:\")\n",
    "    for service, stat in status.items():\n",
    "        print(f\"  {stat} {service}\")\n",
    "    \n",
    "    successful = sum(1 for s in status.values() if '[SUCCESS]' in s)\n",
    "    total = len(status)\n",
    "    print(f\"\\nOverall: {successful}/{total} services connected ({successful/total*100:.1f}%)\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "check_all_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_connections():\n",
    "    status = {}\n",
    "    \n",
    "    # PostgreSQL\n",
    "    try:\n",
    "        pd.read_sql(\"SELECT 1\", pg_engine)\n",
    "        status['PostgreSQL'] = '[SUCCESS]'\n",
    "    except Exception as e:\n",
    "        status['PostgreSQL'] = '[ERROR]'\n",
    "        print(f\"   PostgreSQL error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # ClickHouse\n",
    "    try:\n",
    "        ch_client.query(\"SELECT 1\")\n",
    "        status['ClickHouse'] = '[SUCCESS]'\n",
    "    except Exception as e:\n",
    "        status['ClickHouse'] = '[ERROR]'\n",
    "        print(f\"   ClickHouse error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # MinIO\n",
    "    try:\n",
    "        s3_client.list_buckets()\n",
    "        status['MinIO'] = '[SUCCESS]'\n",
    "    except Exception as e:\n",
    "        status['MinIO'] = '[ERROR]'\n",
    "        print(f\"   MinIO error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Redis\n",
    "    try:\n",
    "        r.ping()\n",
    "        status['Redis'] = '[SUCCESS]'\n",
    "    except Exception as e:\n",
    "        status['Redis'] = '[ERROR]'\n",
    "        print(f\"   Redis error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Kafka\n",
    "    try:\n",
    "        # Test producer connection\n",
    "        producer.bootstrap_connected()\n",
    "        status['Kafka'] = '[SUCCESS]'\n",
    "    except Exception as e:\n",
    "        status['Kafka'] = '[ERROR]'\n",
    "        print(f\"   Kafka error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Trino\n",
    "    try:\n",
    "        test_df = query_trino(\"SELECT 1 as test\")\n",
    "        status['Trino'] = '[SUCCESS]'\n",
    "    except Exception as e:\n",
    "        status['Trino'] = '[ERROR]'\n",
    "        print(f\"   Trino error: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Spark\n",
    "    try:\n",
    "        if 'spark' in globals() and spark is not None:\n",
    "            spark.sql(\"SELECT 1\").collect()\n",
    "            status['Spark'] = '[SUCCESS]'\n",
    "        else:\n",
    "            status['Spark'] = '[ERROR] (Not initialized)'\n",
    "    except Exception as e:\n",
    "        status['Spark'] = '[ERROR]'\n",
    "        print(f\"   Spark error: {str(e)[:50]}...\")\n",
    "    \n",
    "    print(\"Connection Status:\")\n",
    "    for service, stat in status.items():\n",
    "        print(f\"  {stat} {service}\")\n",
    "    \n",
    "    successful = sum(1 for s in status.values() if '[SUCCESS]' in s)\n",
    "    total = len(status)\n",
    "    print(f\"\\nOverall: {successful}/{total} services connected ({successful/total*100:.1f}%)\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "check_all_connections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready-to-Use Code Snippets\n",
    "\n",
    "### Load data from PostgreSQL to Spark\n",
    "```python\n",
    "df_spark = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", POSTGRES_URL.replace(\"postgresql://\", \"jdbc:postgresql://\")) \\\n",
    "    .option(\"dbtable\", \"your_table\") \\\n",
    "    .option(\"user\", os.getenv('POSTGRES_USER', 'admin')) \\\n",
    "    .option(\"password\", os.getenv('POSTGRES_PASSWORD', 'admin')) \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### Save Spark DataFrame to ClickHouse\n",
    "```python\n",
    "# Convert Spark DF to Pandas then to ClickHouse\n",
    "pandas_df = spark_df.toPandas()\n",
    "ch_client.insert_df('your_table', pandas_df)\n",
    "```\n",
    "\n",
    "### Stream data with Kafka\n",
    "```python\n",
    "# Send data\n",
    "send_message('your-topic', {'key': 'value'})\n",
    "\n",
    "# Consume data\n",
    "consumer = create_consumer('your-topic')\n",
    "for message in consumer:\n",
    "    print(message.value)\n",
    "    break  # Process one message\n",
    "```\n",
    "\n",
    "### Cache DataFrame in Redis\n",
    "```python\n",
    "# Cache a DataFrame\n",
    "cache_dataframe('my_data', df, expire_seconds=3600)\n",
    "\n",
    "# Retrieve cached DataFrame\n",
    "cached_df = get_cached_dataframe('my_data')\n",
    "```\n",
    "\n",
    "### Query across multiple databases with Trino\n",
    "```python\n",
    "# Query data from different sources\n",
    "query = '''\n",
    "SELECT pg.*, ch.analytics_column \n",
    "FROM postgresql.public.user_data pg\n",
    "JOIN clickhouse.analytics.user_events ch \n",
    "ON pg.user_id = ch.user_id\n",
    "'''\n",
    "result_df = query_trino(query)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
