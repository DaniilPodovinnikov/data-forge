x-airflow-common: &airflow-common
  build:
    context: ./infra/airflow
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@postgres/metastore
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@postgres/metastore
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
  volumes:
    - ./infra/airflow/dags:/opt/airflow/dags
    - ./infra/airflow/logs:/opt/airflow/logs
    - ./infra/airflow/plugins:/opt/airflow/plugins
  user: "50000:0"
  depends_on:
    redis:
      condition: service_started
    postgres:
      condition: service_started
services:
  minio:
    build:
      context: ./infra/minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles: [core]

  hive-metastore:
    build:
      context: ./infra/hive-metastore
    environment:
      - HIVE_METASTORE_DB_TYPE=postgres
      - HIVE_METASTORE_DB_HOST=postgres
      - HIVE_METASTORE_DB_NAME=metastore
      - HIVE_METASTORE_DB_USER=admin
      - HIVE_METASTORE_DB_PASS=admin
    depends_on:
      - postgres
    ports:
      - "9083:9083"
    healthcheck:
      test: ["CMD", "pidof", "java"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    profiles: [core]

  trino:
    build:
      context: ./infra/trino
    ports:
      - "8080:8080"
    volumes:
      - ./infra/trino/config:/etc/trino
      - trino-data:/var/trino/data
    user: "0:0"
    depends_on:
      - hive-metastore
      - minio
    profiles: [core]


  postgres:
    build:
      context: ./infra/postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pg-data:/var/lib/postgresql/data
      - ./infra/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 10s
      retries: 5
      start_period: 5s
    profiles: [core,airflow]

  debezium:
    build:
      context: ./infra/debezium
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
    depends_on:
      - kafka
      - postgres
    ports:
      - "8083:8083"
    profiles: [core]

  kafka:
    build:
      context: ./infra/kafka
    environment:
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: broker,controller
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@kafka:9093
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
    ports:
      - "9092:9092"
    volumes:
      - kafka-data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    profiles: [core]

  schema-registry:
    build:
      context: ./infra/schema-registry
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka:9092
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    depends_on:
      - kafka
    ports:
      - "8081:8081"
    profiles: [core]

  kafka-ui:
    build:
      context: ./infra/kafka-ui
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_CLUSTERS_0_SCHEMA_REGISTRY: http://schema-registry:8081
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8082:8080"
    profiles: [core]

  spark-master:
    build:
      context: ./infra/spark
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8088:8080"
    volumes:
      - ./processing/spark/jobs:/opt/spark/jobs
    healthcheck:
      test: ["CMD", "pidof", "java"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      minio:
        condition: service_healthy
      hive-metastore:
        condition: service_healthy
      kafka:
        condition: service_healthy
    profiles: [core]

  spark-worker-1:
    build:
      context: ./infra/spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    healthcheck:
      test: ["CMD", "curl", "-f", "http://spark-master:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      spark-master:
        condition: service_healthy
    profiles: [core]

  spark-worker-2:
    build:
      context: ./infra/spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    healthcheck:
      test: ["CMD", "curl", "-f", "http://spark-master:8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      spark-master:
        condition: service_healthy
    profiles: [core]

  redis:
    build:
      context: ./infra/redis
    expose:
      - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    profiles: [core,airflow]



  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    ports:
      - "8085:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    profiles: [airflow]

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    profiles: [airflow]

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: "0"
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      airflow-apiserver:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    profiles: [airflow]

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 1024M
        reservations:
          cpus: '0.50'
          memory: 512M
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    profiles: [airflow]

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          export AIRFLOW_UID=$(id -u)
        fi
        mkdir -p /opt/airflow/{logs,dags,plugins}
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        airflow db migrate
        airflow users create --username airflow --password airflow --firstname Airflow --lastname Admin --role Admin --email airflow@airflow.com
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
    user: "0:0"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    profiles: [airflow]

  superset:
    build:
      context: ./infra/superset
    environment:
      - SUPERSET_SECRET_KEY=your_superset_key
      - SUPERSET_ADMIN_USERNAME=admin
      - SUPERSET_ADMIN_PASSWORD=admin
      - SUPERSET_ADMIN_FIRSTNAME=Admin
      - SUPERSET_ADMIN_LASTNAME=User
      - SUPERSET_ADMIN_EMAIL=admin@superset.com
    entrypoint: ["/app/pythonpath/create_admin.sh"]
    ports:
      - "8089:8088"
    volumes:
      - ./infra/superset/provisioning:/app/provisioning
      - ./infra/superset/create_admin.sh:/app/pythonpath/create_admin.sh
      - ./docker:/app/docker
    depends_on:
      - trino
    profiles: [core]

  jupyterlab:
    build:
      context: ./infra/jupyterlab
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./processing/spark/jobs:/home/jovyan/spark-jobs
    environment:

      - JUPYTER_ENABLE_LAB=yes
    profiles: [explore]

volumes:
  minio-data:
  pg-data:
  kafka-data:
  trino-data:
